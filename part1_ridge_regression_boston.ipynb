{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 Baseline vs attribute LR vs full LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('http://www0.cs.ucl.ac.uk/staff/M.Herbster/boston-filter/Boston-filtered.csv') \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CRIM',\n",
       " ' ZN ',\n",
       " 'INDUS ',\n",
       " 'CHAS',\n",
       " 'NOX',\n",
       " 'RM',\n",
       " 'AGE',\n",
       " 'DIS',\n",
       " 'RAD',\n",
       " 'TAX',\n",
       " 'PTRATIO',\n",
       " 'LSTAT']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_col = 'MEDV'\n",
    "X_cols = df.loc[:, df.columns != Y_col].columns\n",
    "features = list(X_cols)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'MEDV'\n",
    "def get_train_test(df,split_proportion=2/3):\n",
    "    \"\"\"\n",
    "     Splits data into train and test sets based on split_proportion. \n",
    "     Args:\n",
    "     \t df: pandas dataframe to be split\n",
    "     \t split_proportion: proportion of all samples to be used for training\n",
    "     \n",
    "     Returns: \n",
    "     \t trainX, trainY, testX, testY\n",
    "    \"\"\"\n",
    "    shuffledidx = np.random.permutation(df.index)\n",
    "    m = len(df.index)\n",
    "    nsamples_train = int(split_proportion*m)\n",
    "    train = df.iloc[shuffledidx[:nsamples_train]]\n",
    "    test = df.iloc[shuffledidx[nsamples_train:]]\n",
    "    \n",
    "    trainY = train[target].to_numpy()\n",
    "    trainX = train.loc[:, train.columns != target].to_numpy() #.columns\n",
    "    testY = test[target].to_numpy()\n",
    "    testX = test.loc[:, test.columns != target].to_numpy()\n",
    "\n",
    "    return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "     Compute mean squared error ( MSE ) between two sets of labels\n",
    "     Args:\n",
    "     \t y_true: Ground truth ( correct ) target values.\n",
    "     \t y_pred: Estimated target values.\n",
    "     Returns: \n",
    "     \t Mean squared error\n",
    "    \"\"\"    \n",
    "    assert y_true.shape == y_pred.shape\n",
    "    return np.sum((y_true-y_pred)**2) / len(y_true)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Regression\n",
    "\n",
    "def NaiveRegressionCustom(xvals, yvals):\n",
    "    \"\"\"\n",
    "     Naive regression as a baseline\n",
    "     Args:\n",
    "     \t xvals: array of independent vars\n",
    "     \t yvals: array of target vars\n",
    "     Returns: \n",
    "     \t regression weights\n",
    "    \"\"\"\n",
    "    # feature matrix is a vector of ones\n",
    "    xvals = np.ones((xvals.shape[0],1))\n",
    "    # Linear Regression Fitting : W = (X.T X)^-1 X.T Y\n",
    "    wts = np.linalg.pinv(xvals.T @ xvals) @ xvals.T @ yvals\n",
    "    return wts\n",
    "\n",
    "def predict_NaiveRegressionCustom(xvals,wts):\n",
    "    \"\"\"     \n",
    "     Args:\n",
    "     \t xvals: x values to predict on\n",
    "     \t wts: regression weights\n",
    "     Returns: \n",
    "     \t predicted target variable\n",
    "    \"\"\"\n",
    "    xvals = np.ones((xvals.shape[0], 1)) # feature matrix is a vector of ones\n",
    "    return wts @ xvals.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22.25845697]\n",
      "22.258456973293768\n",
      "Train MSE:80.85144925111607, Test MSE: 91.76001423632094\n"
     ]
    }
   ],
   "source": [
    "trainX, trainY, testX, testY = get_train_test(df)\n",
    "# Fitting\n",
    "naive_wts = NaiveRegressionCustom(trainX, trainY)\n",
    "# Prediction\n",
    "train_pred = predict_NaiveRegressionCustom(trainX,naive_wts)\n",
    "train_mse = compute_mse(trainY, train_pred)\n",
    "test_pred = predict_NaiveRegressionCustom(testX,naive_wts)\n",
    "test_mse = compute_mse(testY, test_pred)\n",
    "print(naive_wts) # weights are the average of training examples\n",
    "print(np.average(trainY))\n",
    "print(f'Train MSE: {train_mse}, Test MSE: {test_mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 runs\n",
    "naive_train_MSE_list = []\n",
    "naive_test_MSE_list = []\n",
    "for i in range(20):\n",
    "    trainX, trainY, testX, testY = get_train_test(df)\n",
    "    # Fitting\n",
    "    naive_wts = NaiveRegressionCustom(trainX, trainY)\n",
    "    # Prediction\n",
    "    train_pred = predict_NaiveRegressionCustom(trainX,naive_wts)\n",
    "    train_mse = compute_mse(trainY, train_pred)\n",
    "    naive_train_MSE_list.append(train_mse)\n",
    "    test_pred = predict_NaiveRegressionCustom(testX,naive_wts)\n",
    "    test_mse = compute_mse(testY, test_pred)\n",
    "    naive_test_MSE_list.append(test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive regression train MSE mean : 84.11766709225228, SD: 5.5152352796306445\n",
      "Naive regression test MSE mean: 85.2898626427924, SD: 10.858148273396655\n"
     ]
    }
   ],
   "source": [
    "print(f'Naive regression train MSE mean : {np.mean(naive_train_MSE_list)}, SD: {np.std(naive_train_MSE_list)}')\n",
    "print(f'Naive regression test MSE mean: {np.mean(naive_test_MSE_list)}, SD: {np.std(naive_test_MSE_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4b) the constant function predicts a single, constant value for all observations corresponding to the average (or mean) of the target variable across all observations in each split of training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c. Linear Regression with single attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearRegressionCustom(xvals, yvals):\n",
    "    \"\"\"\n",
    "     implementation of LinearRegression to get fitting weights\n",
    "     Args:\n",
    "     \t xvals: array of independent vars\n",
    "     \t yvals: array of target vars\n",
    "     Returns: \n",
    "     \t  regression weights   \n",
    "    \"\"\"\n",
    "    # incorporating a bias term\n",
    "    xvals = np.concatenate([np.ones((xvals.shape[0], 1)), xvals], axis=1)\n",
    "    wts = np.linalg.pinv(xvals.T @ xvals) @ xvals.T @ yvals\n",
    "    return wts\n",
    "def predict_LinearRegressionCustom(xvals,wts):\n",
    "    \"\"\"\n",
    "     Predict using custom linear regression.\n",
    "     Args:\n",
    "     \t xvals: x values to predict on\n",
    "     \t wts: regression weights\n",
    "     Returns: \n",
    "     \t predicted target variable\n",
    "    \"\"\"\n",
    "    xvals = np.concatenate([np.ones((xvals.shape[0], 1)), xvals], axis=1) # incorporating a bias term\n",
    "    return wts @ xvals.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE list {0: 69.64729125303994, 1: 71.10816230694698, 2: 61.81213104404266, 3: 80.85820807654265, 4: 64.33607732185811, 5: 38.63929130897324, 6: 68.22314611823643, 7: 75.9298501897251, 8: 68.95025592606095, 9: 63.51807183070817, 10: 64.02331921551655, 11: 38.76004981809042}\n",
      "Test MSE list {0: 76.14221498496215, 1: 78.74896321121062, 2: 71.26348436103665, 3: 84.3684254027199, 4: 79.22115806111282, 5: 53.714996435865615, 6: 81.65933573327132, 7: 86.41429353181879, 8: 78.94387523246141, 9: 71.07814735897044, 10: 60.49822031127982, 11: 38.34869030829268}\n"
     ]
    }
   ],
   "source": [
    "train_attribute_MSE_all = {}\n",
    "test_attribute_MSE_all = {}\n",
    "# Linear Regression with single attributes\n",
    "trainX, trainY, testX, testY = get_train_test(df)\n",
    "# Iterating over each attribute\n",
    "for idx in range(trainX.shape[-1]):\n",
    "    attribute_train = np.array(trainX[:,idx]).reshape(-1,1)\n",
    "    # Fitting\n",
    "    attribute_weights = LinearRegressionCustom(attribute_train,trainY)\n",
    "    # Prediction\n",
    "    train_pred = predict_LinearRegressionCustom(attribute_train, attribute_weights)\n",
    "    train_mse = compute_mse(trainY, train_pred)\n",
    "    train_attribute_MSE_all[idx] = train_mse\n",
    "    attribute_test = np.array(testX[:,idx]).reshape(-1,1)\n",
    "    test_pred = predict_LinearRegressionCustom(attribute_test,attribute_weights)\n",
    "    test_mse = compute_mse(testY, test_pred)\n",
    "    test_attribute_MSE_all[idx] = test_mse\n",
    "\n",
    "print('Train MSE list', train_attribute_MSE_all)\n",
    "print('Test MSE list',test_attribute_MSE_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_attribute_MSE_all = {}\n",
    "test_attribute_MSE_all = {}\n",
    "for idx in range(trainX.shape[-1]):\n",
    "    train_attribute_MSE_all[idx] = []\n",
    "    test_attribute_MSE_all[idx] = []\n",
    "\n",
    "#Averaging LR_attribute over 20 runs\n",
    "for i in range(20):\n",
    "    trainX, trainY, testX, testY = get_train_test(df)\n",
    "    # Iterating over each attribute\n",
    "    for idx in range(trainX.shape[-1]):\n",
    "        attribute_train = np.array(trainX[:,idx]).reshape(-1,1)\n",
    "        # Fitting\n",
    "        attribute_weights = LinearRegressionCustom(attribute_train,trainY)\n",
    "        # Prediction\n",
    "        train_pred = predict_LinearRegressionCustom(attribute_train, attribute_weights)\n",
    "        train_mse = compute_mse(trainY, train_pred)\n",
    "        train_attribute_MSE_all[idx].append(train_mse)\n",
    "        attribute_test = np.array(testX[:,idx]).reshape(-1,1)\n",
    "        test_pred = predict_LinearRegressionCustom(attribute_test,attribute_weights)\n",
    "        test_mse = compute_mse(testY, test_pred)\n",
    "        test_attribute_MSE_all[idx].append(test_mse)\n",
    "\n",
    "# print('Train MSE list', train_attribute_MSE_all)\n",
    "# print('Test MSE list',test_attribute_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Train attributes mean:  {0: 70.26090906829704, 1: 72.31070306085113, 2: 64.02609127416903, 3: 81.67081960794172, 4: 68.30497035493883, 5: 43.88462150179403, 6: 71.64602224313069, 7: 78.54903652501866, 8: 71.08905370463242, 9: 65.01840012093386, 10: 61.51873615519669, 11: 37.74620075614458}\n",
      "MSE Train attributes sd:  {0: 4.278819239766112, 1: 5.422247381609391, 2: 5.381477811530468, 3: 4.549784344920677, 4: 5.351383124916789, 5: 3.698192573821819, 6: 5.213095758706833, 7: 5.600702344983223, 8: 5.328209419615138, 9: 5.1250134157394776, 10: 4.038658558064396, 11: 2.6062808861447744}\n",
      "MSE Test attributes mean:  {0: 76.72145655869566, 1: 76.42389112299614, 2: 66.45737529404474, 3: 82.97951322030502, 4: 70.8534141925231, 5: 43.497518645679165, 6: 74.44725612022852, 7: 80.98063110962056, 8: 74.75020433022763, 9: 68.18308340162815, 10: 65.53295732819566, 11: 40.20386482826912}\n",
      "MSE Test attributes sd:  {0: 8.35451544901033, 1: 11.035109447380817, 2: 10.964209406194767, 3: 9.415171341987275, 4: 10.848834594532576, 5: 7.441814622164257, 6: 10.507608559640925, 7: 11.462159648879615, 8: 10.848488545398993, 9: 10.515209956729159, 10: 8.185848194218904, 11: 5.315408063579914}\n"
     ]
    }
   ],
   "source": [
    "# 5d\n",
    "mean_MSE_train_attribute = {}\n",
    "mean_MSE_test_attribute = {}\n",
    "sd_MSE_train_attribute = {}\n",
    "sd_MSE_test_attribute = {}\n",
    "\n",
    "for key, values in train_attribute_MSE_all.items():\n",
    "    mean_MSE_train_attribute[key] = np.mean(values) \n",
    "    sd_MSE_train_attribute[key] = np.std(values)\n",
    "for key, values in test_attribute_MSE_all.items():\n",
    "    mean_MSE_test_attribute[key] = np.mean(values)\n",
    "    sd_MSE_test_attribute[key] = np.std(values) \n",
    "print('MSE Train attributes mean: ',mean_MSE_train_attribute)\n",
    "print('MSE Train attributes sd: ',sd_MSE_train_attribute)\n",
    "print('MSE Test attributes mean: ',mean_MSE_test_attribute)\n",
    "print('MSE Test attributes sd: ',sd_MSE_test_attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4d. Linear Regression using all attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over all attributes: train_mse: 22.247105274926852, test_mse:23.341646765078483\n"
     ]
    }
   ],
   "source": [
    "# Single Run\n",
    "trainX, trainY, testX, testY = get_train_test(df)\n",
    "# Fitting\n",
    "model_weights = LinearRegressionCustom(trainX,trainY)\n",
    "# Prediction\n",
    "train_pred = predict_LinearRegressionCustom(trainX, model_weights)\n",
    "train_mse = compute_mse(trainY, train_pred)\n",
    "test_pred = predict_LinearRegressionCustom(testX,model_weights)\n",
    "test_mse = compute_mse(testY, test_pred)\n",
    "\n",
    "print(f'Over all attributes: train_mse: {train_mse}, test_mse:{test_mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE over all attributes - mean: 21.921379157379935, SD: 1.6187044236477768\n",
      "Test MSE over all attributes - mean: 24.500814188283258, SD: 3.2214767827603064\n"
     ]
    }
   ],
   "source": [
    "#Averaging complete LR over 20 runs\n",
    "train_MSE_list = []\n",
    "test_MSE_list = []\n",
    "for i in range(20):\n",
    "    trainX, trainY, testX, testY = get_train_test(df)\n",
    "    # Fitting\n",
    "    model_weights = LinearRegressionCustom(trainX,trainY)\n",
    "    # Prediction\n",
    "    train_pred = predict_LinearRegressionCustom(trainX, model_weights)\n",
    "    train_mse = compute_mse(trainY, train_pred)\n",
    "    train_MSE_list.append(train_mse)\n",
    "    test_pred = predict_LinearRegressionCustom(testX,model_weights)\n",
    "    test_mse = compute_mse(testY, test_pred)\n",
    "    test_MSE_list.append(test_mse)\n",
    "\n",
    "print(f'Train MSE over all attributes - mean: {np.average(train_MSE_list)}, SD: {np.std(train_MSE_list)}')\n",
    "print(f'Test MSE over all attributes - mean: {np.average(test_MSE_list)}, SD: {np.std(test_MSE_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 Kernelised ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(xi, xj, sigma):\n",
    "    \"\"\"\n",
    "     Gaussian kernel function of two points.\n",
    "    \"\"\"\n",
    "    return np.exp(-(np.linalg.norm(xi - xj)**2 / (2 * (sigma**2))))\n",
    "\n",
    "def compute_kernel_matrix(x1vals,x2vals, sigma):\n",
    "    \"\"\"\n",
    "     Compute the kernel matrix between two sets of points.\n",
    "    \"\"\"\n",
    "    K = np.zeros((x1vals.shape[0], x2vals.shape[0]))\n",
    "    # Computes the Gaussian kernel of the first dimension of the kernel.\n",
    "    for i in range(x1vals.shape[0]):\n",
    "        for j in range(x2vals.shape[0]):\n",
    "            K[i, j] = gaussian_kernel(x1vals[i], x2vals[j], sigma)\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KernelRidgeRegressionCustom(K, yvals, sigma, gamma):\n",
    "    # K = compute_kernel_matrix(xvals,sigma)\n",
    "    # l = xvals.shape[0]\n",
    "    l = K.shape[0]\n",
    "    # alpha* = (K+ gamma.l.I_l)^-1.y\n",
    "    alpha = np.linalg.pinv(K + gamma * l * np.eye(l)) @ yvals\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. kernel ridge regression on the training set \n",
    "using five-fold cross-validation to choose best γ and σ - report training and test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "main_train_X, main_train_Y, testX, testY = get_train_test(df)\n",
    "gamma_values = np.array([2**-i for i in range(40, 25, -1)])\n",
    "sigma_values = np.array([2**i for i in range(7, 14)])\n",
    "\n",
    "# Create folds for cross validation\n",
    "n_folds = 5\n",
    "fold_size = int(len(main_train_X)/n_folds)\n",
    "fold_indices = [] \n",
    "indices = np.arange(len(main_train_X))\n",
    "for i in range(n_folds):\n",
    "    start_idx, end_idx = i * fold_size,  (i + 1) * fold_size\n",
    "    validation_indices = indices[start_idx: end_idx]\n",
    "    train_indices = np.concatenate([indices[:start_idx], indices[end_idx:]])\n",
    "    fold_indices.append((train_indices, validation_indices))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n"
     ]
    }
   ],
   "source": [
    "# dictionary to store kernel matrices for each training fold for each sigma - 5 folds * 7 sigmas\n",
    "kernel_matrices = {}\n",
    "for fold_num, (train_indices, validation_indices) in enumerate(fold_indices):\n",
    "    x_train_fold = main_train_X[train_indices]\n",
    "    # Precompute and store kernel matrix for each sigma for this fold\n",
    "    for sigma in sigma_values:\n",
    "        fold_sigma_key = (fold_num, sigma)\n",
    "        kernel_matrices[fold_sigma_key] = compute_kernel_matrix(x_train_fold,x_train_fold, sigma) \n",
    "    print(\"Fold\",fold_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(t) = \\sum_{i=1}^m \\alpha_i K_{\\beta} (x_i, t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_KernelRidgeRegressionCustom(x_test,x_train,sigma, alpha):\n",
    "    # implementing equation given above\n",
    "    preds = np.zeros(x_test.shape[0])\n",
    "    for i in range(x_test.shape[0]):\n",
    "        prediction = 0\n",
    "        for j in range(x_train.shape[0]):\n",
    "            prediction += alpha[j] * gaussian_kernel(x_train[j], x_test[i], sigma)\n",
    "        preds[i] = prediction\n",
    "    return preds\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma: 9.094947017729282e-13, sigma: 128, scores: 1550.7754405989729\n",
      "gamma: 9.094947017729282e-13, sigma: 256, scores: 350.1663906128115\n",
      "gamma: 9.094947017729282e-13, sigma: 512, scores: 61.4393749552346\n",
      "gamma: 9.094947017729282e-13, sigma: 1024, scores: 16.479885179750966\n",
      "gamma: 9.094947017729282e-13, sigma: 2048, scores: 14.978601179399737\n",
      "gamma: 9.094947017729282e-13, sigma: 4096, scores: 14.121660941104844\n",
      "gamma: 9.094947017729282e-13, sigma: 8192, scores: 15.924566939289061\n",
      "gamma: 1.8189894035458565e-12, sigma: 128, scores: 867.7566093860487\n",
      "gamma: 1.8189894035458565e-12, sigma: 256, scores: 254.64450195295439\n",
      "gamma: 1.8189894035458565e-12, sigma: 512, scores: 36.857594808054905\n",
      "gamma: 1.8189894035458565e-12, sigma: 1024, scores: 15.677637369672993\n",
      "gamma: 1.8189894035458565e-12, sigma: 2048, scores: 14.78169311033462\n",
      "gamma: 1.8189894035458565e-12, sigma: 4096, scores: 13.892947312979704\n",
      "gamma: 1.8189894035458565e-12, sigma: 8192, scores: 17.83876841241208\n",
      "gamma: 3.637978807091713e-12, sigma: 128, scores: 513.240279478584\n",
      "gamma: 3.637978807091713e-12, sigma: 256, scores: 215.58577437643345\n",
      "gamma: 3.637978807091713e-12, sigma: 512, scores: 22.891989159357486\n",
      "gamma: 3.637978807091713e-12, sigma: 1024, scores: 15.260942494839952\n",
      "gamma: 3.637978807091713e-12, sigma: 2048, scores: 14.547896376860155\n",
      "gamma: 3.637978807091713e-12, sigma: 4096, scores: 13.97194717442604\n",
      "gamma: 3.637978807091713e-12, sigma: 8192, scores: 20.07117706339188\n",
      "gamma: 7.275957614183426e-12, sigma: 128, scores: 325.65629968604713\n",
      "gamma: 7.275957614183426e-12, sigma: 256, scores: 180.13372600708013\n",
      "gamma: 7.275957614183426e-12, sigma: 512, scores: 17.1017876695653\n",
      "gamma: 7.275957614183426e-12, sigma: 1024, scores: 14.971644151919985\n",
      "gamma: 7.275957614183426e-12, sigma: 2048, scores: 14.25708739705551\n",
      "gamma: 7.275957614183426e-12, sigma: 4096, scores: 14.565489806873895\n",
      "gamma: 7.275957614183426e-12, sigma: 8192, scores: 22.080204534650626\n",
      "gamma: 1.4551915228366852e-11, sigma: 128, scores: 236.93342031179458\n",
      "gamma: 1.4551915228366852e-11, sigma: 256, scores: 135.07496572242792\n",
      "gamma: 1.4551915228366852e-11, sigma: 512, scores: 15.446587880877098\n",
      "gamma: 1.4551915228366852e-11, sigma: 1024, scores: 14.713800377208893\n",
      "gamma: 1.4551915228366852e-11, sigma: 2048, scores: 13.944520255036371\n",
      "gamma: 1.4551915228366852e-11, sigma: 4096, scores: 15.807387495592854\n",
      "gamma: 1.4551915228366852e-11, sigma: 8192, scores: 23.534169684897456\n",
      "gamma: 2.9103830456733704e-11, sigma: 128, scores: 206.77836529620973\n",
      "gamma: 2.9103830456733704e-11, sigma: 256, scores: 86.9011473670403\n",
      "gamma: 2.9103830456733704e-11, sigma: 512, scores: 15.135249926751722\n",
      "gamma: 2.9103830456733704e-11, sigma: 1024, scores: 14.463210240732659\n",
      "gamma: 2.9103830456733704e-11, sigma: 2048, scores: 13.746654255664188\n",
      "gamma: 2.9103830456733704e-11, sigma: 4096, scores: 17.677950840463915\n",
      "gamma: 2.9103830456733704e-11, sigma: 8192, scores: 24.478263287653412\n",
      "gamma: 5.820766091346741e-11, sigma: 128, scores: 199.88612168336167\n",
      "gamma: 5.820766091346741e-11, sigma: 256, scores: 49.01191067492063\n",
      "gamma: 5.820766091346741e-11, sigma: 512, scores: 14.995937791920412\n",
      "gamma: 5.820766091346741e-11, sigma: 1024, scores: 14.196055108247403\n",
      "gamma: 5.820766091346741e-11, sigma: 2048, scores: 13.873019067145005\n",
      "gamma: 5.820766091346741e-11, sigma: 4096, scores: 19.882377271955914\n",
      "gamma: 5.820766091346741e-11, sigma: 8192, scores: 25.115338541754387\n",
      "gamma: 1.1641532182693481e-10, sigma: 128, scores: 191.6477465180783\n",
      "gamma: 1.1641532182693481e-10, sigma: 256, scores: 27.282721639539453\n",
      "gamma: 1.1641532182693481e-10, sigma: 512, scores: 14.743302248244413\n",
      "gamma: 1.1641532182693481e-10, sigma: 1024, scores: 13.895485208969092\n",
      "gamma: 1.1641532182693481e-10, sigma: 2048, scores: 14.5268309523434\n",
      "gamma: 1.1641532182693481e-10, sigma: 4096, scores: 21.93791697561725\n",
      "gamma: 1.1641532182693481e-10, sigma: 8192, scores: 25.57321205816421\n",
      "gamma: 2.3283064365386963e-10, sigma: 128, scores: 170.73220303566626\n",
      "gamma: 2.3283064365386963e-10, sigma: 256, scores: 17.897009080733397\n",
      "gamma: 2.3283064365386963e-10, sigma: 512, scores: 14.4176496305416\n",
      "gamma: 2.3283064365386963e-10, sigma: 1024, scores: 13.612952729409159\n",
      "gamma: 2.3283064365386963e-10, sigma: 2048, scores: 15.835733003281925\n",
      "gamma: 2.3283064365386963e-10, sigma: 4096, scores: 23.502762765124935\n",
      "gamma: 2.3283064365386963e-10, sigma: 8192, scores: 25.867974292816804\n",
      "gamma: 4.656612873077393e-10, sigma: 128, scores: 136.10093922002415\n",
      "gamma: 4.656612873077393e-10, sigma: 256, scores: 14.653979735883173\n",
      "gamma: 4.656612873077393e-10, sigma: 512, scores: 14.095702810278013\n",
      "gamma: 4.656612873077393e-10, sigma: 1024, scores: 13.501156528065241\n",
      "gamma: 4.656612873077393e-10, sigma: 2048, scores: 17.75992485723909\n",
      "gamma: 4.656612873077393e-10, sigma: 4096, scores: 24.52654098040297\n",
      "gamma: 4.656612873077393e-10, sigma: 8192, scores: 25.99008906349527\n",
      "gamma: 9.313225746154785e-10, sigma: 128, scores: 95.36796360743294\n",
      "gamma: 9.313225746154785e-10, sigma: 256, scores: 13.72348583732421\n",
      "gamma: 9.313225746154785e-10, sigma: 512, scores: 13.772909751518977\n",
      "gamma: 9.313225746154785e-10, sigma: 1024, scores: 13.773094094654567\n",
      "gamma: 9.313225746154785e-10, sigma: 2048, scores: 19.973704095037526\n",
      "gamma: 9.313225746154785e-10, sigma: 4096, scores: 25.125419447076343\n",
      "gamma: 9.313225746154785e-10, sigma: 8192, scores: 26.007558293115107\n",
      "gamma: 1.862645149230957e-09, sigma: 128, scores: 59.52444926533932\n",
      "gamma: 1.862645149230957e-09, sigma: 256, scores: 13.56973468354993\n",
      "gamma: 1.862645149230957e-09, sigma: 512, scores: 13.45011165245008\n",
      "gamma: 1.862645149230957e-09, sigma: 1024, scores: 14.6075221652633\n",
      "gamma: 1.862645149230957e-09, sigma: 2048, scores: 21.966503866393424\n",
      "gamma: 1.862645149230957e-09, sigma: 4096, scores: 25.44694749796097\n",
      "gamma: 1.862645149230957e-09, sigma: 8192, scores: 26.12648525614002\n",
      "gamma: 3.725290298461914e-09, sigma: 128, scores: 35.266613555136345\n",
      "gamma: 3.725290298461914e-09, sigma: 256, scores: 13.651422717290265\n",
      "gamma: 3.725290298461914e-09, sigma: 512, scores: 13.21930289175288\n",
      "gamma: 3.725290298461914e-09, sigma: 1024, scores: 16.06354074918307\n",
      "gamma: 3.725290298461914e-09, sigma: 2048, scores: 23.399476691312184\n",
      "gamma: 3.725290298461914e-09, sigma: 4096, scores: 25.64879958895841\n",
      "gamma: 3.725290298461914e-09, sigma: 8192, scores: 26.602659730806828\n",
      "gamma: 7.450580596923828e-09, sigma: 128, scores: 22.38139928280938\n",
      "gamma: 7.450580596923828e-09, sigma: 256, scores: 13.728515111025605\n",
      "gamma: 7.450580596923828e-09, sigma: 512, scores: 13.245641293607026\n",
      "gamma: 7.450580596923828e-09, sigma: 1024, scores: 18.022851919839397\n",
      "gamma: 7.450580596923828e-09, sigma: 2048, scores: 24.277266831606813\n",
      "gamma: 7.450580596923828e-09, sigma: 4096, scores: 25.937139348024004\n",
      "gamma: 7.450580596923828e-09, sigma: 8192, scores: 27.504993071494642\n",
      "gamma: 1.4901161193847656e-08, sigma: 128, scores: 16.890628364285842\n",
      "gamma: 1.4901161193847656e-08, sigma: 256, scores: 13.65218118594405\n",
      "gamma: 1.4901161193847656e-08, sigma: 512, scores: 13.70007351684618\n",
      "gamma: 1.4901161193847656e-08, sigma: 1024, scores: 20.127511648111476\n",
      "gamma: 1.4901161193847656e-08, sigma: 2048, scores: 24.835889384309553\n",
      "gamma: 1.4901161193847656e-08, sigma: 4096, scores: 26.53331633057163\n",
      "gamma: 1.4901161193847656e-08, sigma: 8192, scores: 28.691272066813944\n",
      "Best gamma and sigma : 3.725290298461914e-09 512 13.21930289175288\n"
     ]
    }
   ],
   "source": [
    "chosen_gamma, chosen_sigma = None, None\n",
    "best_score = float('inf')\n",
    "\n",
    "cv_errors = np.zeros((len(gamma_values), len(sigma_values)))\n",
    "\n",
    "# 525 total - 5 folds * 7 sigma * 15 gamma\n",
    "for i, gamma in enumerate(gamma_values):\n",
    "    for j, sigma in enumerate(sigma_values):\n",
    "        fold_scores = []\n",
    "\n",
    "        for fold_num, (train_indices, validation_indices) in enumerate(fold_indices):\n",
    "            x_train, y_train = main_train_X[train_indices], main_train_Y[train_indices]\n",
    "            x_validation, y_validation = main_train_X[validation_indices], main_train_Y[validation_indices]\n",
    "            # Retrieve the precomputed kernel matrix for this fold and sigma\n",
    "            K_train = kernel_matrices[(fold_num, sigma)]\n",
    "            \n",
    "            # Train the model on the training data\n",
    "            alpha = KernelRidgeRegressionCustom(K_train, y_train, sigma, gamma)\n",
    "            \n",
    "            # Make predictions on the validation data\n",
    "            y_pred = predict_KernelRidgeRegressionCustom(x_validation, x_train, sigma, alpha)\n",
    "            score = compute_mse(y_validation, y_pred)\n",
    "            fold_scores.append(score)\n",
    "            # print(y_validation, y_pred)\n",
    "\n",
    "        average_score = np.mean(fold_scores)\n",
    "        print(f'gamma: {gamma}, sigma: {sigma}, scores: {average_score}')\n",
    "        cv_errors[i, j] = average_score\n",
    "\n",
    "        if average_score < best_score:\n",
    "            best_score = average_score\n",
    "            chosen_gamma, chosen_sigma = gamma, sigma\n",
    "print('Best gamma and sigma :',chosen_gamma,chosen_sigma,best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-28.0, 9.0, 13.21930289175288)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log2(chosen_gamma), np.log2(chosen_sigma), best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "log2_gammas = np.log2(gamma_values)\n",
    "log2_sigma = np.log2(sigma_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 b) Plot the “cross-validation error” (mean over folds of validation error) as a function of γ and σ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqEAAAIhCAYAAAB+EmOGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABqVElEQVR4nO3deZyNdf/H8fcxmM3MMJZZGGPsskcJ3SEhyZJKqCzToqbubqkGKUYxMpUULbcS4hZ1h7TcspUS1djKlrbBJJPbNiNmMXO+vz/85tyOOcM4zjbm9Xw8rked73Wd7/mc75w58/G5ru/3shhjjAAAAAAPKuftAAAAAFD2kIQCAADA40hCAQAA4HEkoQAAAPA4klAAAAB4HEkoAAAAPI4kFAAAAB5HEgoAAACPIwkFAACAx5GEXqQffvhBw4cPV1xcnAICAlSpUiVdeeWVSklJ0dGjR70dntfUqVNHw4YNsz3eu3evLBaL5s6de8HnJiUlyWKxOPW6Cxcu1PTp0x3us1gsSkpKcqrfSzF37lxZLJZity+++MLjMfmKNWvWqG3btgoODpbFYtGyZcu8FsuuXbuUlJSkvXv3Ftk3bNgw1alTx+MxXYyjR49q4MCBqlGjhiwWi/r163fe461WqxYsWKAePXqoRo0aqlChgipXrqxrrrlGL7zwgg4fPuyZwMuwkn4npaenKyEhQQ0bNlRgYKDCw8PVvHlz3XfffUpPT7cddynfnYAvKO/tAEqTN998UwkJCWrUqJGeeOIJXXHFFTp9+rQ2bdqkN954Qxs3btTSpUu9HaZPiIqK0saNG1WvXj23vs7ChQu1Y8cOjRw5ssi+jRs3qlatWm59/fOZM2eOGjduXKT9iiuu8EI03meM0YABA9SwYUMtX75cwcHBatSokdfi2bVrlyZOnKjOnTsXSTiffvpp/eMf//BOYCX07LPPaunSpXr77bdVr149hYeHF3tsdna2+vbtq9WrV+uOO+7QK6+8oujoaGVlZWnDhg16/vnn9eGHH+qrr77y4DuAI7///ruuvPJKVa5cWY899pgaNWqkzMxM7dq1S++9955+++03xcTESJLuvfde3XjjjV6OGHAeSWgJbdy4UQ8++KC6deumZcuWyd/f37avW7dueuyxx7RixYrz9pGdna3AwEB3h+oT/P39dc0113g1Bm+/frNmzdS2bduLeo4xRjk5OQ4/J9nZ2QoICLikysepU6cUFBTk9PMvxR9//KGjR4/qlltuUdeuXb0SQ0m5+x9PrrBjxw7Vq1dPd9555wWPHTlypFatWqWFCxdq0KBBdvtuvvlmPfXUU/rXv/7lrlBxEd58800dPnxY3333neLi4mzt/fr105NPPimr1Wprq1Wrllf/oQ1cKk7Hl1BycrIsFotmzZpll4AWqlixovr06WN7XKdOHd18881asmSJWrdurYCAAE2cOFHSmT8effv2VZUqVRQQEKBWrVpp3rx5dv1ZrVZNmjRJjRo1UmBgoCpXrqwWLVro5Zdfth3z3//+V/fff79iYmLk7++v6tWrq2PHjlq9enWx7+P06dOqUaOG7r777iL7jh8/rsDAQI0aNUqSlJOTo8cee0ytWrVSWFiYwsPD1b59e3344YcXHK/iTsd/8sknatWqlfz9/RUXF6cXXnjB4fNfffVVXXfddapRo4aCg4PVvHlzpaSk6PTp07ZjOnfurE8++UT79u2zO91dyNGpr5KM/RdffCGLxaJ3331X48aNU3R0tEJDQ3XDDTdoz549F3zvF8Nisejhhx/WG2+8oSZNmsjf31/z5s2zndJfuXKl4uPjVb16dQUFBSk3N1dWq1UpKSlq3Lix/P39VaNGDQ0ZMkS///67Xd+dO3dWs2bN9OWXX6pDhw4KCgpSfHx8sbFs2rRJAwcOVJ06dRQYGKg6depo0KBB2rdvn91xp06d0uOPP267JCU8PFxt27bVu+++W2zfSUlJtj+Wo0ePlsVisVUfizv17ehUY+F4zZ8/X02aNFFQUJBatmypjz/+uMjzf/zxRw0aNEgRERHy9/dX7dq1NWTIEOXm5mru3Lm6/fbbJUldunSxfXYKP6+OYsrJydHYsWMVFxenihUrqmbNmnrooYd0/Phxu+MKf/dXrFihK6+8UoGBgWrcuLHefvvtYsfnbEePHlVCQoJq1qypihUrqm7duho3bpxyc3Ml/e93a/Xq1dq9e/cFL/M4ePCg3n77bfXq1atIAlooKChI9913n11bSX4Hpf99zjZu3KgOHTrYPjtz5syRdOZ3/sorr1RQUJCaN29e5B/rhT/nH374Qbfffrvtu2bUqFHKz8/Xnj17dOONNyokJER16tRRSkqK3fMv5XtKklatWqW+ffuqVq1aCggIUP369TVixIgilycUxrlz504NGjRIYWFhioiIUHx8vDIzM+2OzcrK0n333aeqVauqUqVKuvHGG/XTTz+VKJ4jR46oXLlyqlGjhsP95cr978+2o9+R3NxcPfbYY4qMjFRQUJCuu+46bd68ucglU4XfMWvXrrXFGhoaqiFDhujkyZPKyMjQgAEDVLlyZUVFRenxxx8v8rOfOHGi2rVrp/DwcIWGhurKK6/U7NmzZYwp0XsFqISWQEFBgdauXas2bdrYToOUxJYtW7R792499dRTiouLU3BwsPbs2aMOHTqoRo0aeuWVV1S1alUtWLBAw4YN059//qnExERJUkpKipKSkvTUU0/puuuu0+nTp/Xjjz/a/cG7++67tWXLFk2ePFkNGzbU8ePHtWXLFh05cqTYmCpUqKC77rpLb7zxhl599VWFhoba9r377rvKycnR8OHDJZ35Mjt69Kgef/xx1axZU3l5eVq9erX69++vOXPmaMiQIRc1jmvWrFHfvn3Vvn17LVq0SAUFBUpJSdGff/5Z5Nhff/1VgwcPtv3B//777zV58mT9+OOPtj/mr732mu6//379+uuvJboMoqRjX+jJJ59Ux44d9dZbbykrK0ujR49W7969tXv3bvn5+V3w9QoKCpSfn2/XZrFYijx32bJl+uqrrzR+/HhFRkaqRo0aSk1NlSTFx8erV69emj9/vk6ePKkKFSrowQcf1KxZs/Twww/r5ptv1t69e/X000/riy++0JYtW1StWjVb3wcPHtRdd92lxMREJScn2/0BO9fevXvVqFEjDRw4UOHh4Tp48KBef/11XXXVVdq1a5et31GjRmn+/PmaNGmSWrdurZMnT2rHjh3n/dzde++9atmypfr376+///3vGjx4sMN/zJXEJ598otTUVD3zzDOqVKmSUlJSdMstt2jPnj2qW7euJOn777/Xtddeq2rVqumZZ55RgwYNdPDgQS1fvlx5eXnq1auXkpOT9eSTT+rVV1/VlVdeKan4CqgxRv369dOaNWs0duxY/e1vf9MPP/ygCRMmaOPGjdq4caPd+/n+++/12GOPacyYMYqIiNBbb72le+65R/Xr19d1111X7HvLyclRly5d9Ouvv2rixIlq0aKFvvrqK02ZMkXbtm3TJ598YrvUJSEhQZmZmbYKZnGXeXz++efKz8+3+0dySZTkd7BQRkaGhg8frsTERNWqVUszZsxQfHy80tPT9e9//1tPPvmkwsLC9Mwzz6hfv3767bffFB0dbdfHgAEDdNddd2nEiBFatWqVLeFdvXq1EhIS9Pjjj2vhwoUaPXq06tevr/79+0u69O+pX3/9Ve3bt9e9996rsLAw7d27V9OmTdO1116r7du3q0KFCnbH33rrrbrjjjt0zz33aPv27Ro7dqwk2cak8LOyYcMGjR8/XldddZW+/vpr9ezZs0Tj3r59e7366qvq37+/Ro0apfbt29t9T1/I8OHDtXjxYiUmJur666/Xrl27dMsttygrK8vh8ffee6/69++vRYsWaevWrXryySdtyX///v11//33a/Xq1Zo6daqio6NtRQrpzHfGiBEjVLt2bUnSN998o7///e86cOCAxo8fX+KYUYYZXFBGRoaRZAYOHFji58TGxho/Pz+zZ88eu/aBAwcaf39/s3//frv2nj17mqCgIHP8+HFjjDE333yzadWq1Xlfo1KlSmbkyJEljqnQDz/8YCSZWbNm2bVfffXVpk2bNsU+Lz8/35w+fdrcc889pnXr1nb7YmNjzdChQ22P09LSjCQzZ84cW1u7du1MdHS0yc7OtrVlZWWZ8PBwc76PYkFBgTl9+rR55513jJ+fnzl69KhtX69evUxsbKzD50kyEyZMsD0u6dh//vnnRpK56aab7I577733jCSzcePGYmM1xpg5c+YYSQ43Pz+/IjGGhYXZvaez+xgyZIhd++7du40kk5CQYNf+7bffGknmySeftLV16tTJSDJr1qw5b7zFyc/PN3/99ZcJDg42L7/8sq29WbNmpl+/fhfdX+Fn4vnnn7drHzp0qMOf4YQJE4p8LiSZiIgIk5WVZWvLyMgw5cqVM1OmTLG1XX/99aZy5crm0KFDxcbz/vvvG0nm888/L7Lv3JhWrFhhJJmUlBS74xYvXlzkdyk2NtYEBASYffv22dqys7NNeHi4GTFiRLHxGGPMG2+8YSSZ9957z6596tSpRpJZuXKlra1Tp06madOm5+3PGGOee+45I8msWLGiyL7Tp0/bbcU53+9g4eds06ZNtrYjR44YPz8/ExgYaA4cOGBr37Ztm5FkXnnlFVtb4c/5xRdftHvNVq1aGUlmyZIldvFWr17d9O/fv9hYz/c9dSFWq9WcPn3a7Nu3z0gyH374YZE4z/0MJCQkmICAAGO1Wo0xxvznP/8xkux+Z4wxZvLkyUW+k4qLYcSIEaZcuXJGkrFYLKZJkybm0UcfNWlpaXbHnvs7snPnTiPJjB492u64d99910iy+44u/I75+9//bndsv379jCQzbdo0u/ZWrVqZK6+8sti4Cz8jzzzzjKlataptPIDz4XS8G7Vo0UINGza0a1u7dq26du1apKI6bNgwnTp1Shs3bpQkXX311fr++++VkJCgzz77zOG/Yq+++mrNnTtXkyZN0jfffFPkVIkxRvn5+XabJDVv3lxt2rSxnS6TpN27d+u7774rcrr2/fffV8eOHVWpUiWVL19eFSpU0OzZs7V79+6LGouTJ08qNTVV/fv3V0BAgK09JCREvXv3LnL81q1b1adPH1WtWlV+fn6qUKGChgwZooKCghKf1jpXSce+0LmVoxYtWkhSkdPTxXnnnXeUmppqt3377bdFjrv++utVpUoVh33ceuutdo8///xzW8xnu/rqq9WkSROtWbPGrr1KlSq6/vrrSxTvX3/9ZasylS9fXuXLl1elSpV08uRJu5/31Vdfrf/85z8aM2aMvvjiC2VnZ5eof1fp0qWLQkJCbI8jIiJUo0YN28/l1KlTWrdunQYMGKDq1au75DXXrl0rqei433777QoODi4y7q1atbJVhyQpICBADRs2vOBnZ+3atQoODtZtt91m1174uue+zqXYtm2bKlSoYLedfQr6Yn4Ho6Ki1KZNG9vj8PBw1ahRQ61atbKreDZp0kSS49+hm2++2e5xkyZNZLFY7CqI5cuXV/369Ys8/1K+pw4dOqQHHnhAMTExtufGxsZKksPnO/peyMnJ0aFDhyT973f03Gt1Bw8efMFYpDNnS9544w399ttveu211zR8+HCdPn1aL730kpo2bap169YV+9zCfQMGDLBrv+2221S+vOMTn47GXZJ69epVpP3ccV+7dq1uuOEGhYWF2T4j48eP15EjR2zjAZwPSWgJVKtWTUFBQUpLS7uo50VFRRVpO3LkiMP2wi/qwlOaY8eO1QsvvKBvvvlGPXv2VNWqVdW1a1dt2rTJ9pzFixdr6NCheuutt9S+fXuFh4dryJAhysjIkCTNmzevyB+ZQvHx8dq4caN+/PFHSWdmcvv7+9tdM7ZkyRINGDBANWvW1IIFC7Rx40alpqYqPj5eOTk5FzUWx44dk9VqVWRkZJF957bt379ff/vb33TgwAG9/PLL+uqrr5SamqpXX31VkpxOeko69oWqVq1q97jwdGtJX79JkyZq27at3Xb2H+pCjmIqbl9hjMW9j3Pfw/n6PtfgwYM1c+ZM3Xvvvfrss8/03XffKTU1VdWrV7d7z6+88opGjx6tZcuWqUuXLgoPD1e/fv30888/l/i1LsW5PxfpzM+mMMZjx46poKDApRM2jhw5ovLlyxdJai0WiyIjIy/42Tk3xvO9TmRkZJHr/GrUqKHy5cuf95KH4hQmw+cmEI0aNbL94+jc60Ev9nfQ0cz8ihUrFmmvWLGiJDn8/nB0bFBQkN0/Wgvbz37+pXxPWa1Wde/eXUuWLFFiYqLWrFmj7777Tt98843D9yld+Huh8LNy7nGOvvvOJzY2Vg8++KBmz56tn3/+WYsXL1ZOTo6eeOKJYp9T+PmIiIiwa3cUT6HifkaO2s8ez++++07du3eXdGYy1ddff63U1FSNGzdOkvPf0yhbuCa0BPz8/NS1a1f95z//0e+//17iP26OZjFXrVpVBw8eLNL+xx9/SJLturvy5ctr1KhRGjVqlI4fP67Vq1frySefVI8ePZSenq6goCBVq1ZN06dP1/Tp07V//34tX75cY8aM0aFDh7RixQr17t3bdm3huQYNGqRRo0Zp7ty5mjx5subPn69+/frZVeQWLFiguLg4LV682O69FE6QuBhVqlSRxWKxJchnO7dt2bJlOnnypJYsWWKrSEhnKjeXoqRj72nnm+1+7r7CPyQHDx4s8jn8448/iryHks6kz8zM1Mcff6wJEyZozJgxtvbC6+3OFhwcrIkTJ2rixIn6888/bVXR3r172/5RczECAgIcfqacXbcyPDxcfn5+RSZqXYqqVasqPz9f//3vf+0SUWOMMjIydNVVV7nsdb799lsZY+x+docOHVJ+fr5Tn9HOnTurfPnyWr58ue6//35be2BgoG31hnMndrnrd9AdLuV7aseOHfr+++81d+5cDR061Nb+yy+/OB1P4WflyJEjdomfo+++izFgwABNmTJFO3bsOO9rS9Kff/6pmjVr2toL43GlRYsWqUKFCvr444/t/qHgzbV/UfpQCS2hsWPHyhij++67T3l5eUX2nz59Wh999NEF++natavWrl1rS3wKvfPOOwoKCnK4rFDlypV122236aGHHtLRo0cdLq5du3ZtPfzww+rWrZu2bNki6cwX0rmVuEJVqlRRv3799M477+jjjz9WRkZGkVPxFotFFStWtPtiz8jIKPGs07MFBwfr6quv1pIlS+z+NX3ixIki41b4emdP9DDG6M033yzSb0mqS4WcGXtfU3hqfcGCBXbtqamp2r17t9NLH1ksFhljikwWeuutt1RQUFDs8yIiIjRs2DANGjRIe/bs0alTpy76tevUqaNDhw7ZTVDLy8vTZ599dtF9SWeSq06dOun9998/byJ7MZXtwnE9d9w/+OADnTx50mVLTnXt2lV//fVXkT/k77zzjl0cFyMqKkrx8fH65JNPtGjRohI952J+B73tUr6nHL1PSfrnP//pdDxdunSRpCJLXi1cuLBEz3f0D2XpzOUy6enpRSZ0na1w0tvixYvt2v/9738XmSR5qSwWi8qXL2830TI7O1vz58936evg8kYltITat2+v119/XQkJCWrTpo0efPBBNW3aVKdPn9bWrVs1a9YsNWvWzOH1jWebMGGCPv74Y3Xp0kXjx49XeHi4/vWvf+mTTz5RSkqKwsLCJEm9e/e2rTNZvXp17du3T9OnT1dsbKwaNGigzMxMdenSRYMHD1bjxo0VEhKi1NRUrVixwjZr9ELi4+O1ePFiPfzww6pVq5ZuuOEGu/2FS0wlJCTotttuU3p6up599llFRUU5der12Wef1Y033mhbV7WgoEBTp05VcHCwXbWtW7duqlixogYNGqTExETl5OTo9ddf17Fjx4r02bx5cy1ZskSvv/662rRpo3LlyhW7NmdJx95VduzY4fCLv169ek5fq9ioUSPdf//9mjFjhsqVK6eePXvaZsfHxMTo0Ucfdarf0NBQXXfddXr++edVrVo11alTR+vWrdPs2bNVuXJlu2PbtWunm2++WS1atFCVKlW0e/duzZ8/X+3bt3dqDdI77rhD48eP18CBA/XEE08oJydHr7zyynmT3wspnN3crl07jRkzRvXr19eff/6p5cuX65///KdCQkLUrFkzSdKsWbMUEhKigIAAxcXFOTxt2a1bN/Xo0UOjR49WVlaWOnbsaJsd37p1a4dLnjljyJAhevXVVzV06FDt3btXzZs31/r165WcnKybbrqpyO9oSU2fPl1paWm68847tXz5cvXt21fR0dE6deqUfvzxRy1atEgBAQG2S3Yu5nfQ2y7le6px48aqV6+exowZI2OMwsPD9dFHH2nVqlVOx9O9e3ddd911SkxM1MmTJ9W2bVt9/fXXJU7OJk+erK+//lp33HGHWrVqpcDAQKWlpWnmzJk6cuSInn/++WKf27RpUw0aNEgvvvii/Pz8dP3112vnzp168cUXFRYWdt7VMS5Wr169NG3aNA0ePFj333+/jhw5ohdeeMHpVS9QRnlvTlTptG3bNjN06FBTu3ZtU7FiRRMcHGxat25txo8fbzcTNzY21vTq1cthH9u3bze9e/c2YWFhpmLFiqZly5Z2s8iNMebFF180HTp0MNWqVTMVK1Y0tWvXNvfcc4/Zu3evMcaYnJwc88ADD5gWLVqY0NBQExgYaBo1amQmTJhgTp48WaL3UlBQYGJiYowkM27cOIfHPPfcc6ZOnTrG39/fNGnSxLz55psOZy2XZHa8McYsX77ctGjRwvaennvuOYf9ffTRR6Zly5YmICDA1KxZ0zzxxBO2Wadnz2Y+evSoue2220zlypWNxWKx60cOZqKWZOwLZ8e///77du3FvadznW92vCTz5ptv2sX40EMPFdtHampqkX0FBQVm6tSppmHDhqZChQqmWrVq5q677jLp6el2x5V09nSh33//3dx6662mSpUqJiQkxNx4441mx44dRX62Y8aMMW3btjVVqlQx/v7+pm7duubRRx81hw8fPm//xc2ON8aYTz/91LRq1coEBgaaunXrmpkzZxY7O97ReJ0bozHG7Nq1y9x+++2matWqts/bsGHDTE5Oju2Y6dOnm7i4OOPn52f3s3U0Yz87O9uMHj3axMbGmgoVKpioqCjz4IMPmmPHjhWJxdHvfqdOnUynTp2KH6D/d+TIEfPAAw+YqKgoU758eRMbG2vGjh1rF3dhfxfz8y0oKDDvvPOO6datm6lWrZopX768CQsLM1dffbV5+umnze+//253fEl/B4uLo7hxOPdnWPhz/u9//2t33NChQ01wcHCR5zt6vZJ+Tzmya9cu061bNxMSEmKqVKlibr/9drN///4i3x/FxVn4u3r2zPXjx4+b+Ph4U7lyZRMUFGS6detmfvzxxxLNjv/mm2/MQw89ZFq2bGnCw8ONn5+fqV69urnxxhvNp59+aneso/eYk5NjRo0aZWrUqGECAgLMNddcYzZu3GjCwsLMo48+WiTuc79jLubn8fbbb5tGjRrZvgemTJliZs+eXWQ8gOJYjGFVWQAALlcbNmxQx44d9a9//avEs/QBTyAJBQDgMrFq1Spt3LhRbdq0UWBgoL7//ns999xzCgsL0w8//FBktQHAm7gmFACAy0RoaKhWrlyp6dOn68SJE6pWrZp69uypKVOmkIDC51AJBQAAgMd5dYmmL7/8Ur1791Z0dLQsFkuRZUmSkpLUuHFjBQcHq0qVKrrhhhsc3nEGAADgcnShXMkYo6SkJEVHRyswMFCdO3fWzp077Y7Jzc3V3//+d1WrVk3BwcHq06ePS9dSdpZXk9CTJ0+qZcuWmjlzpsP9DRs21MyZM7V9+3atX79ederUUffu3fXf//7Xw5ECAAB43oVypZSUFE2bNk0zZ85UamqqIiMj1a1bN504ccJ2zMiRI7V06VItWrRI69ev119//aWbb775kpbCcwWfOR1vsVi0dOlS9evXr9hjsrKyFBYWptWrV7tscWgAAIDS4NxcyRij6OhojRw5UqNHj5Z0puoZERGhqVOnasSIEcrMzFT16tU1f/583XHHHZLO3GEvJiZGn376qXr06OGtt1N6Jibl5eVp1qxZCgsLU8uWLYs9Ljc31+52bVarVUePHlXVqlVLfAtDAADgXcYYnThxQtHR0S5daL+kcnJyHN4h0RXMObfmlc7cuetiF/tPS0tTRkaGunfvbtdPp06dtGHDBo0YMUKbN2/W6dOn7Y6Jjo5Ws2bNtGHDBpLQ8/n44481cOBAnTp1SlFRUVq1atV57588ZcoUTZw40YMRAgAAd0lPT1etWrU8+po5OTmKi62kjEPuOV1dqVIl/fXXX3ZtEyZMUFJS0kX1k5GRIenMLZTPFhERoX379tmOqVixoqpUqVLkmMLne4vPJ6FdunTRtm3bdPjwYb355psaMGCAvv32W9WoUcPh8WPHjtWoUaNsjzMzM1W7dm11Chmg8paKngq71Cg48deFDwLOUT460tsh+CxrlRBvh+CzTEW/Cx9URlkr+vyfY4/Lz8/V15ueV0iI53+n8vLylHGoQPs211FoiGursFknrIpts1fp6ekKDQ21tV/KLU/Prao6qrSeqyTHuJvPf+qDg4NVv3591a9fX9dcc40aNGig2bNna+zYsQ6PL66cXd5SkSTUAYulgrdDQClUvhz3hy6O1Y+xKY7x8/k/OV5jLc/YFMebiVKlEIsqhbj29a06019oaKhdEuqMyMgzBYGMjAxFRUXZ2g8dOmSrjkZGRiovL0/Hjh2zq4YeOnRIHTp0uKTXv1RenR3vDGOM3TWfAAAA7lBgrG7ZXCUuLk6RkZFatWqVrS0vL0/r1q2zJZht2rRRhQoV7I45ePCgduzY4fUk1Kv/9Prrr7/0yy+/2B6npaVp27ZtCg8PV9WqVTV58mT16dNHUVFROnLkiF577TX9/vvvuv32270YNQAAgGecL1eqXbu2Ro4cqeTkZDVo0EANGjRQcnKygoKCNHjwYElSWFiY7rnnHj322GOqWrWqwsPD9fjjj6t58+a64YYbvPW2JHk5Cd20aZO6dOlie1x4LefQoUP1xhtv6Mcff9S8efN0+PBhVa1aVVdddZW++uorNW3a1FshAwCAMsIqI6tcu5LlxfZ3vlxp7ty5SkxMVHZ2thISEnTs2DG1a9dOK1eutLuW9qWXXlL58uU1YMAAZWdnq2vXrpo7d678/Lx7nbbPrBPqLoVri3YNvYtrQh0oOGsxW6CkyteM9nYIPssazsSk4hgm3xTL6s/YnCs/P0frvpmkzMzMS7528mIV5g4Ze2q7ZWJSZKP9XnlfvoZPPQAAgANWWeW6Kzj/1yfOKHUTkwAAAFD6UQkFAABwoMAYFbj4qkVX91eaUQkFAACAx1EJBQAAcMAXZsdfzkhCAQAAHLDKqIAk1G04HQ8AAACPoxIKAADgAKfj3YtKKAAAADyOSigAAIADLNHkXlRCAQAA4HFUQgEAAByw/v/m6j5xBpVQAAAAeByVUAAAAAcK3LBOqKv7K81IQgEAABwoMGc2V/eJMzgdDwAAAI+jEgoAAOAAE5Pci0ooAAAAPI5KKAAAgANWWVQgi8v7xBlUQgEAAOBxVEIBAAAcsJozm6v7xBlUQgEAAOBxVEIBAAAcKHDDNaGu7q80KztJaPWqkp+/t6PwPSdOeDsC32U4Z1Ick5Pj7RB8lqWgkrdD8FmGVbqLZSlg4Z5z+cKYkIS6F6fjAQAA4HFlpxIKAABwEazGIqtx8RJNLu6vNKMSCgAAAI+jEgoAAOAA14S6F5VQAAAAeByVUAAAAAcKVE4FLq7XFbi0t9KNSigAAAA8jkooAACAA8YNs+MNs+NtSEIBAAAcYGKSe3E6HgAAAB5HJRQAAMCBAlNOBcbFE5O4e60NlVAAAAB4HJVQAAAAB6yyyOriep1VlEILUQkFAACAx1EJBQAAcIDZ8e5FJRQAAAAeRyUUAADAAffMjuea0EIkoQAAAA6cmZjk2tPnru6vNON0PAAAgA87ceKERo4cqdjYWAUGBqpDhw5KTU217TfGKCkpSdHR0QoMDFTnzp21c+dOL0ZcMiShAAAADlhVTgUu3pxZ8unee+/VqlWrNH/+fG3fvl3du3fXDTfcoAMHDkiSUlJSNG3aNM2cOVOpqamKjIxUt27ddOLECVcPiUuRhAIAAPio7OxsffDBB0pJSdF1112n+vXrKykpSXFxcXr99ddljNH06dM1btw49e/fX82aNdO8efN06tQpLVy40NvhnxdJKAAAgAOFE5NcvUlSVlaW3Zabm+swhvz8fBUUFCggIMCuPTAwUOvXr1daWpoyMjLUvXt32z5/f3916tRJGzZscN/guABJKAAAgIfFxMQoLCzMtk2ZMsXhcSEhIWrfvr2effZZ/fHHHyooKNCCBQv07bff6uDBg8rIyJAkRURE2D0vIiLCts9XMTseAADAAauT13Cev88zSzSlp6crNDTU1u7v71/sc+bPn6/4+HjVrFlTfn5+uvLKKzV48GBt2bLFdozFYj/r3hhTpM3XUAkFAADwsNDQULvtfElovXr1tG7dOv31119KT0/Xd999p9OnTysuLk6RkZGSVKTqeejQoSLVUV9DEgoAAOBAgbG4ZXNWcHCwoqKidOzYMX322Wfq27evLRFdtWqV7bi8vDytW7dOHTp0cMUwuA2n4wEAABwoXFbJtX1e/B2TPvvsMxlj1KhRI/3yyy964okn1KhRIw0fPlwWi0UjR45UcnKyGjRooAYNGig5OVlBQUEaPHiwS2N3NZJQAAAAH5aZmamxY8fq999/V3h4uG699VZNnjxZFSpUkCQlJiYqOztbCQkJOnbsmNq1a6eVK1cqJCTEy5GfH0koAACAA1ZTTlYX3zve6sS94wcMGKABAwYUu99isSgpKUlJSUmXEJnncU0oAAAAPK7MVEJPNK2m8hUCLnxgGRP0S5q3Q0ApZLJzvB2Cz7JYrd4OwWdZnKgAlRn5fG7OZSnw/pj4yjWhlysqoQAAAPC4MlMJBQAAuBhW6ZKWVCquT5xBJRQAAAAeRyUUAADAAffctpP6XyGSUAAAAAcKTDkVuHiJJlf3V5oxEgAAAPA4KqEAAAAOWGWRVa6emOTa/kozKqEAAADwOCqhAAAADnBNqHsxEgAAAPA4KqEAAAAOuOe2ndT/Cnl1JL788kv17t1b0dHRslgsWrZsmW3f6dOnNXr0aDVv3lzBwcGKjo7WkCFD9Mcff3gvYAAAALiEV5PQkydPqmXLlpo5c2aRfadOndKWLVv09NNPa8uWLVqyZIl++ukn9enTxwuRAgCAssZqLG7ZcIZXT8f37NlTPXv2dLgvLCxMq1atsmubMWOGrr76au3fv1+1a9f2RIgAAABwg1J1TWhmZqYsFosqV65c7DG5ubnKzc21Pc7KyvJAZAAA4HJjdcM1ody2839KzUjk5ORozJgxGjx4sEJDQ4s9bsqUKQoLC7NtMTExHowSAABcLqymnFs2nFEqRuL06dMaOHCgrFarXnvttfMeO3bsWGVmZtq29PR0D0UJAACAkvL50/GnT5/WgAEDlJaWprVr1563CipJ/v7+8vf391B0AADgclUgiwpcfJtNV/dXmvl0ElqYgP7888/6/PPPVbVqVW+HBAAAABfwahL6119/6ZdffrE9TktL07Zt2xQeHq7o6Gjddttt2rJliz7++GMVFBQoIyNDkhQeHq6KFSt6K2wAAFAGuOMaTq4J/R+vJqGbNm1Sly5dbI9HjRolSRo6dKiSkpK0fPlySVKrVq3snvf555+rc+fOngoTAAAALubVJLRz584yxhS7/3z7AAAA3KlArr+Gs8ClvZVu1IQBAADgcT49MQkAAMBbuCbUvUhCAQAAHCgw5VTg4qTR1f2VZowEAAAAPI5KKAAAgANGFlldPDHJsFi9DZVQAAAAeByVUAAAAAe4JtS9GAkAAAB4XJmphP7RxahcIIvfn6vhUm9HgNLI5J32dgg+q1wuY1Mc41/B2yH4Lm7OUpQPDInVWGQ1rr2G09X9lWZUQgEAAOBxZaYSCgAAcDEKVE4FLq7Xubq/0owkFAAAwAFOx7sX6TgAAAA8jkooAACAA1aVk9XF9TpX91eaMRIAAADwOCqhAAAADhQYiwpcfA2nq/srzaiEAgAAwOOohAIAADjA7Hj3ohIKAADgo/Lz8/XUU08pLi5OgYGBqlu3rp555hlZrVbbMcYYJSUlKTo6WoGBgercubN27tzpxahLhiQUAADAAWPKyerizZiLS72mTp2qN954QzNnztTu3buVkpKi559/XjNmzLAdk5KSomnTpmnmzJlKTU1VZGSkunXrphMnTrh6SFyK0/EAAAAOFMiiArl4YtJF9rdx40b17dtXvXr1kiTVqVNH7777rjZt2iTpTBV0+vTpGjdunPr37y9JmjdvniIiIrRw4UKNGDHCpfG7EpVQAAAAD8vKyrLbcnNzHR537bXXas2aNfrpp58kSd9//73Wr1+vm266SZKUlpamjIwMde/e3fYcf39/derUSRs2bHD/G7kEVEIBAAAcsBrXTySymjP/jYmJsWufMGGCkpKSihw/evRoZWZmqnHjxvLz81NBQYEmT56sQYMGSZIyMjIkSREREXbPi4iI0L59+1wau6uRhAIAAHhYenq6QkNDbY/9/f0dHrd48WItWLBACxcuVNOmTbVt2zaNHDlS0dHRGjp0qO04i8U+WTbGFGnzNSShAAAADhROJnJ1n5IUGhpql4QW54knntCYMWM0cOBASVLz5s21b98+TZkyRUOHDlVkZKSkMxXRqKgo2/MOHTpUpDrqa7gmFAAAwEedOnVK5crZp2t+fn62JZri4uIUGRmpVatW2fbn5eVp3bp16tChg0djvVhUQgEAABywyiKri2fHX2x/vXv31uTJk1W7dm01bdpUW7du1bRp0xQfHy/pzGn4kSNHKjk5WQ0aNFCDBg2UnJysoKAgDR482KWxuxpJKAAAgI+aMWOGnn76aSUkJOjQoUOKjo7WiBEjNH78eNsxiYmJys7OVkJCgo4dO6Z27dpp5cqVCgkJ8WLkF0YSCgAA4ECBsajAxbPjL7a/kJAQTZ8+XdOnTy/2GIvFoqSkJIez630ZSSgAAIAD7pyYBCYmAQAAwAuohAIAADhglcX1i9W7eKJTaUYlFAAAAB5HJRQAAMAB44YlmgyVUBsqoQAAAPA4KqEAAAAOWI0brgl1cX+lGZVQAAAAeByVUAAAAAdYJ9S9SEIBAAAc4HS8e5WZJPSj7jMVEsK/Ps71gP7m7RB8l4UviuKY/NPeDsF35eZ5OwLfZQK9HYHvsvJ9U4TV2wHA3cpMEgoAAHAxrG5YoonF6v+H0iAAAAA8jkooAACAA1wT6l5UQgEAAOBxVEIBAAAcoBLqXlRCAQAA4HFUQgEAABygEupeJKEAAAAOkIS6F6fjAQAA4HFUQgEAABwwcv3i8salvZVuVEIBAADgcVRCAQAAHOCaUPeiEgoAAACPoxIKAADgAJVQ96ISCgAAAI+jEgoAAOAAlVD3IgkFAABwgCTUvTgdDwAAAI+jEgoAAOCAMRYZF1cuXd1faUYlFAAAAB5HJRQAAMABqywuv22nq/srzaiEAgAAwOOohAIAADjA7Hj3ohIKAAAAj6MSCgAA4ACz492LSigAAAA8zqtJ6JdffqnevXsrOjpaFotFy5Yts9u/ZMkS9ejRQ9WqVZPFYtG2bdu8EicAACh7Cq8JdfWGM7yahJ48eVItW7bUzJkzi93fsWNHPffccx6ODAAAlHWFp+NdveEMr14T2rNnT/Xs2bPY/Xfffbckae/evR6KCAAAAJ5w2U1Mys3NVW5uru1xVlaWJKlOhUoKrcAlsOeylK/g7RB8liko8HYIvsvK2BTH5OR4OwTfVRDq7Qh8F3+efJJxw+lzKqH/c9l97KdMmaKwsDDbFhMT4+2QAAAAcI7LLgkdO3asMjMzbVt6erq3QwIAAKWQkWSMizdvvykfctklof7+/goNDbXbAAAASqM6derIYrEU2R566CFJkjFGSUlJio6OVmBgoDp37qydO3d6OeqSueySUAAAAFewyuKW7WKkpqbq4MGDtm3VqlWSpNtvv12SlJKSomnTpmnmzJlKTU1VZGSkunXrphMnTrhkDE6fPq3hw4frt99+c0l/Z/NqEvrXX39p27ZttvU/09LStG3bNu3fv1+SdPToUW3btk27du2SJO3Zs0fbtm1TRkaGt0IGAAC4ZFlZWXbb2ZOqz1a9enVFRkbato8//lj16tVTp06dZIzR9OnTNW7cOPXv31/NmjXTvHnzdOrUKS1cuNAlcVaoUEFLly51SV/n8moSumnTJrVu3VqtW7eWJI0aNUqtW7fW+PHjJUnLly9X69at1atXL0nSwIED1bp1a73xxhteixkAAJQN7lwnNCYmxm4i9ZQpUy4YT15enhYsWKD4+HhZLBalpaUpIyND3bt3tx3j7++vTp06acOGDS4bh1tuuaXIDYVcwatLNHXu3FnGFH+J7rBhwzRs2DDPBQQAAPD/rMYii4uXVCpc8ik9Pd1u3oq/v/8Fn7ts2TIdP37clhsVnhmOiIiwOy4iIkL79u1zUcRS/fr19eyzz2rDhg1q06aNgoOD7fY/8sgjTvV72a0TCgAA4OucmTw9e/Zs9ezZU9HR0XbtFot9omyMKdJ2Kd566y1VrlxZmzdv1ubNm4u8NkkoAACACxUuq+TqPp2xb98+rV69WkuWLLG1RUZGSjpTEY2KirK1Hzp0qEh19FKkpaW5rK+zMTseAADAx82ZM0c1atSwzZORpLi4OEVGRtpmzEtnrhtdt26dOnTo4JY4jDHnvZTyYpCEAgAAOODOiUkXw2q1as6cORo6dKjKl//fSWyLxaKRI0cqOTlZS5cu1Y4dOzRs2DAFBQVp8ODBrhwKvfPOO2revLkCAwMVGBioFi1aaP78+ZfUJ6fjAQAAfNjq1au1f/9+xcfHF9mXmJio7OxsJSQk6NixY2rXrp1WrlypkJAQl73+tGnT9PTTT+vhhx9Wx44dZYzR119/rQceeECHDx/Wo48+6lS/JKEAAAAOOFu5vFCfF6t79+7FngK3WCxKSkpSUlLSJUZWvBkzZuj111/XkCFDbG19+/ZV06ZNlZSU5HQSyul4AAAAFOvgwYMOrzHt0KGDDh486HS/JKEAAAAOWI3FLVtpU79+fb333ntF2hcvXqwGDRo43S+n4wEAABzwpSWavGnixIm644479OWXX6pjx46yWCxav3691qxZ4zA5LSkqoQAAACjWrbfequ+++07VqlXTsmXLtGTJElWrVk3fffedbrnlFqf7pRIKAADgwJlKqKsnJrm0O7c7ffq07r//fj399NNasGCBS/umEgoAAACHKlSooKVLl7qlb5JQAAAAB3xlsXpvu+WWW7Rs2TKX98vpeAAAABSrfv36evbZZ7Vhwwa1adNGwcHBdvsfeeQRp/olCQUAAHDA/P/m6j5Lm7feekuVK1fW5s2btXnzZrt9FouFJBQAAACuZYzR559/rho1aigoKMilfXNNKAAAgANcE3omCW3YsKEOHDjg8r5JQgEAABwxbtpKkXLlyqlBgwY6cuSI6/t2eY8AAAC4bKSkpOiJJ57Qjh07XNov14QCAAA44o7T56XsdLwk3XXXXTp16pRatmypihUrKjAw0G7/0aNHneqXJBQAAADFmj59ulv6JQkFAABw4MxtO13fZ2kzdOhQt/TLNaEAAAAo4r333lNeXp7t8d69e1VQUGB7fOrUKaWkpDjdP5XQMq5caCVvh+CzrJlZ3g7BZ5W2JUY8yWTneDsEn2U5648X7JkKft4OAQ64Y0ml0vT9OWjQIB08eFA1atSQJLVo0ULbtm1T3bp1JUknTpzQ2LFjlZiY6FT/VEIBAABQhDnn2oFzH18qKqEAAACOGIvrZ7OXokqou5GEAgAAOMDEJPciCQUAAIBDn332mcLCwiRJVqtVa9assS1af/z48UvqmyQUAADAEXfcZrOUVULPXZ5pxIgRdo8tFucvLyAJBQAAQBFWq9Wt/ZOEAgAAOFDWl2hyN5ZoAgAAgMdRCQUAAChOKbuGszShEgoAAACPoxIKAADgANeEuhdJKAAAgCMs0WQnLy9Phw4dKjJrvnbt2k71RxIKAACAYv3888+Kj4/Xhg0b7NqNMbJYLCooKHCqX5JQAAAAhyz/v7m6z9Jl2LBhKl++vD7++GNFRUVd0gL1ZyMJBQAAQLG2bdumzZs3q3Hjxi7tlyQUAADAEa4JlSRdccUVOnz4sMv7ZYkmAAAAFGvq1KlKTEzUF198oSNHjigrK8tucxaVUAAAAEeohEqSbrjhBklS165d7dqZmAQAAAC3+fzzz93SL0koAACAI8ZyZnN1n6VMp06d3NIvSSgAAIADxpzZXN1naXT8+HHNnj1bu3fvlsVi0RVXXKH4+HiFhYU53ScTkwAAAFCsTZs2qV69enrppZd09OhRHT58WNOmTVO9evW0ZcsWp/ulEgoAAOAIE5MkSY8++qj69OmjN998U+XLn0kd8/Pzde+992rkyJH68ssvneqXJBQAAADF2rRpk10CKknly5dXYmKi2rZt63S/nI4HAABwpHBikqu3i3TgwAHdddddqlq1qoKCgtSqVStt3rz5f2Eao6SkJEVHRyswMFCdO3fWzp07XTYMoaGh2r9/f5H29PR0hYSEON0vSSgAAICPOnbsmDp27KgKFSroP//5j3bt2qUXX3xRlStXth2TkpKiadOmaebMmUpNTVVkZKS6deumEydOuCSGO+64Q/fcc48WL16s9PR0/f7771q0aJHuvfdeDRo0yOl+OR0PAADggMWc2Vzd58WYOnWqYmJiNGfOHFtbnTp1bP9vjNH06dM1btw49e/fX5I0b948RUREaOHChRoxYsQlx/zCCy/IYrFoyJAhys/PlyRVqFBBDz74oJ577jmn+72kJHTXrl3av3+/8vLy7Nr79OlzKd0CAABc1s693aW/v7/8/f2LHLd8+XL16NFDt99+u9atW6eaNWsqISFB9913nyQpLS1NGRkZ6t69u11fnTp10oYNG1yShFasWFEvv/yypkyZol9//VXGGNWvX19BQUGX1K9TSehvv/2mW265Rdu3b5fFYpH5/0WvLJYz1zk4e/smAAAAn+HG2fExMTF2zRMmTFBSUlKRw3/77Te9/vrrGjVqlJ588kl99913euSRR+Tv768hQ4YoIyNDkhQREWH3vIiICO3bt8+loQcFBal58+Yu68+pJPQf//iH4uLitHr1atWtW1ffffedjhw5oscee0wvvPCCy4IDAADwGjfeMSk9PV2hoaG2ZkdVUEmyWq1q27atkpOTJUmtW7fWzp079frrr2vIkCG24woLgbaX+f/7ujurf//+mjt3rkJDQ22n+YuzZMkSp17DqSR048aNWrt2rapXr65y5cqpXLlyuvbaazVlyhQ98sgj2rp1q1PBwPMK6tX0dgg+y7L1L2+H4MM421Ecc87lSfifcrmnvR2Cz7L4V/B2CD7HUlpvLVRCoaGhdklocaKionTFFVfYtTVp0kQffPCBJCkyMlKSlJGRoaioKNsxhw4dKlIdvRhhYWG2JDY0NPSSEtriOJWEFhQUqFKlSpKkatWq6Y8//lCjRo0UGxurPXv2uDRAAAAAr/CBxeo7duxYJLf66aefFBsbK0mKi4tTZGSkVq1apdatW0uS8vLytG7dOk2dOtXpMM+eCDV37lyn+zkfp5ZoatasmX744QdJUrt27ZSSkqKvv/5azzzzjOrWrevSAAEAAMqqRx99VN98842Sk5P1yy+/aOHChZo1a5YeeughSWdOw48cOVLJyclaunSpduzYoWHDhikoKEiDBw92SQzXX3+9jh8/XqQ9KytL119/vdP9OlUJfeqpp3Ty5ElJ0qRJk3TzzTfrb3/7m6pWrarFixc7HQwAAIDP8IFK6FVXXaWlS5dq7NixeuaZZxQXF6fp06frzjvvtB2TmJio7OxsJSQk6NixY2rXrp1Wrlx5SQvJn+2LL74oshKSJOXk5Oirr75yul+nktAePXrY/r9u3bratWuXjh49qipVqrjlmgEAAICy6uabb9bNN99c7H6LxaKkpCSHs+svReFZb+nMspyFM/GlM5dmrlixQjVrOj+3xGWL1YeHh7uqKwAAAO/zgUqoN7Vq1UoWi0UWi8XhaffAwEDNmDHD6f6dSkJzcnI0Y8YMff755zp06JCsVqvd/i1btjgdEAAAALwvLS1NxhjbcpzVq1e37atYsaJq1KghPz8/p/t3KgmNj4/XqlWrdNttt+nqq6/mFDwAALj8uHGd0NKgcAb+ucVGV3EqCf3kk0/06aefqmPHjq6OBwAAAD7I1bdrdyoJrVmzpstmXAEAAPgiizmzubrP0sZdt2t3ap3QF198UaNHj3b5PUkBAAB8hnHTVsoU3q79zz//VFBQkHbu3Kkvv/xSbdu21RdffOF0v05VQtu2baucnBzVrVtXQUFBqlDB/nZjR48edTogAAAA+A533a7dqSR00KBBOnDggJKTkxUREeH0xKQvv/xSzz//vDZv3qyDBw9q6dKl6tevn22/MUYTJ07UrFmzbIuvvvrqq2ratKlTrwcAAICL467btTuVhG7YsEEbN25Uy5YtnX5hSTp58qRatmyp4cOH69Zbby2yPyUlRdOmTdPcuXPVsGFDTZo0Sd26ddOePXu4JhUAAMADCm/XXrduXdvt2itWrKhZs2Zd0u3anUpCGzdurOzsbKdftFDPnj3Vs2dPh/uMMZo+fbrGjRun/v37S5LmzZuniIgILVy4UCNGjLjk1wcAACiORW6YmOTa7jzCXbdrdyoJfe655/TYY49p8uTJat68eZFrQkNDQ50OqFBaWpoyMjLUvXt3W5u/v786deqkDRs2FJuE5ubmKjc31/Y4KyvrkmMBAAAoq9x1u3anktAbb7xRktS1a1e7dmOMLBaL01P1z1Z4f9KIiAi79oiIiPPOyp8yZYomTpx4ya8PAADKuDK+WP35uOJ27U4loZ9//vklv3BJnZthFya6xRk7dqxGjRple5yVlaWYmBi3xQcAAHC5KbwUsiSWLFni1Gs4lYR26tTJqRe7GJGRkZLOVESjoqJs7YcOHSpSHT2bv7+//P393R4fAAC4zLljXc9Ssk5oWFiY7f+NMVq6dKnCwsLUtm1bSdLmzZt1/Pjxi0pWz+VUEvrDDz84bLdYLAoICFDt2rUvORGMi4tTZGSkVq1apdatW0uS8vLytG7dOk2dOvWS+gYAALigMpyEzpkzx/b/o0eP1oABA/TGG2/Iz89P0pllmxISEi5pHpBTSWirVq3Oe0q8QoUKuuOOO/TPf/5TAQEBxR73119/6ZdffrE9TktL07Zt2xQeHq7atWtr5MiRSk5OVoMGDdSgQQMlJycrKChIgwcPdiZsAAAAXKS3335b69evtyWgkuTn56dRo0apQ4cOev75553q16nbdi5dulQNGjTQrFmztG3bNm3dulWzZs1So0aNtHDhQs2ePVtr167VU089dd5+Nm3apNatW9sqnaNGjVLr1q01fvx4SVJiYqJGjhyphIQEtW3bVgcOHNDKlStZIxQAALhd4b3jXb2VNvn5+dq9e3eR9t27d8tqtTrdr1OV0MmTJ+vll1+2m7LfokUL1apVS08//bS+++47BQcH67HHHtMLL7xQbD+dO3eWMcX/NCwWi5KSkpSUlORMmAAAALhEw4cPV3x8vH755Rddc801kqRvvvlGzz33nIYPH+50v04lodu3b1dsbGyR9tjYWG3fvl3SmVP2Bw8edDowAAAAryrD14Se7YUXXlBkZKReeuklW24XFRWlxMREPfbYY07369Tp+MaNG+u5555TXl6ere306dN67rnn1LhxY0nSgQMHzjuLHQAAAL6vXLlySkxM1IEDB3T8+HEdP35cBw4cUGJiot11ohfLqUroq6++qj59+qhWrVpq0aKFLBaLfvjhBxUUFOjjjz+WJP32229KSEhwOjAAAACvohJahCvuilnIqSS0Q4cO2rt3rxYsWKCffvpJxhjddtttGjx4sG3S0N133+2yIAEAAOA5V155pdasWaMqVaqodevW510VacuWLU69hlNJqCRVqlRJDzzwgLNPBwAA8GnumM1eWmbH9+3b17bme79+/dzyGiVOQpcvX66ePXuqQoUKWr58+XmP7dOnzyUHBgAA4FVl+N7xEyZMcPj/rlTiJLRfv37KyMhQjRo1zpsRWywWFRQUuCI2AAAAXKZKnISevRjppSxMCgAAUCqU4YlJVapUOe91oGc7evSoU69xUdeEfvvttzp69Kh69uxpa3vnnXc0YcIEnTx5Uv369dOMGTMu+b7x8Jzfu3L3qeLEfF86Tpl4hcWp1d3KBMOZoOLl5l34mLLKBHo7At9znpvZwP2mT5/u9te4qCQ0KSlJnTt3tiWh27dv1z333KNhw4apSZMmev755xUdHc0djgAAQKlXlicmDR061O2vcVFJ6LZt2/Tss8/aHi9atEjt2rXTm2++KUmKiYnRhAkTSEIBAAAuQ9nZ2Tp9+rRdm7Nrh17UObVjx47Z3QVp3bp1uvHGG22Pr7rqKqWnpzsVCAAAgE8xbtpKmZMnT+rhhx9WjRo1VKlSJVWpUsVuc9ZFJaERERFKS0uTJOXl5WnLli1q3769bf+JEydUoUIFp4MBAACAb0lMTNTatWv12muvyd/fX2+99ZYmTpyo6OhovfPOO073e1Gn42+88UaNGTNGU6dO1bJlyxQUFKS//e1vtv0//PCD6tWr53QwAAAAPsMN14SWxkroRx99pHfeeUedO3dWfHy8/va3v6l+/fqKjY3Vv/71L915551O9XtRldBJkybJz89PnTp10ptvvqk333xTFStWtO1/++231b17d6cCAQAA8Cmcjpd0ZgmmuLg4SWeu/yxckunaa6/Vl19+6XS/F1UJrV69ur766itlZmaqUqVK8vPzs9v//vvvq1KlSk4HAwAAAN9St25d7d27V7Gxsbriiiv03nvv6eqrr9ZHH32kypUrO92vU4v9hYWFFUlAJSk8PNyuMgoAAFBqUQmVJA0fPlzff/+9JGns2LG2a0MfffRRPfHEE073e1GVUAAAAJQtjz76qO3/u3Tpoh9//FGbNm1SvXr11LJlS6f7JQkFAABwoCwvVn+2vXv3qk6dOrbHtWvXVu3atS+5X+69BwAAgGLVrVtX1157rf75z386fZ94R0hCAQAAUKxNmzapffv2mjRpkqKjo9W3b1+9//77ys3NvaR+SUIBAABQrCuvvFLPP/+89u/fr//85z+qUaOGRowYoRo1aig+Pt7pfklCAQAAHGF2vB2LxaIuXbrozTff1OrVq1W3bl3NmzfP6f5IQgEAABwonJjk6q20Sk9PV0pKilq1aqWrrrpKwcHBmjlzptP9kYQCAAD4qKSkJFksFrstMjLStt8Yo6SkJEVHRyswMFCdO3fWzp07XRrDrFmz1KlTJ8XFxWnevHkaMGCAfv31V61fv14PPvig0/2ShAIAABTHB07FN23aVAcPHrRt27dvt+1LSUnRtGnTNHPmTKWmpioyMlLdunXTiRMnnHsxB5599lldffXV2rRpk3bu3Kknn3zSbskmZ7FOKAAAgA8rX768XfWzkDFG06dP17hx49S/f39J0rx58xQREaGFCxdqxIgRLnn9/fv3y2KxSJK+/vprtW3bVv7+/pfcL5VQAAAAR9w4MSkrK8tuO99yRz///LOio6MVFxengQMH6rfffpMkpaWlKSMjQ927d7cd6+/vr06dOmnDhg2uGgVbAipJPXv21IEDB1zSL0koAACAh8XExCgsLMy2TZkyxeFx7dq10zvvvKPPPvtMb775pjIyMtShQwcdOXJEGRkZkqSIiAi750RERNj2uZoxrptZxel4AAAAB9x528709HSFhoba2os7vd2zZ0/b/zdv3lzt27dXvXr1NG/ePF1zzTVn+jyrUimdSRTPbfNFVEIBAAA8LDQ01G4r6TWWwcHBat68uX7++WfbdaLnVj0PHTpUpDrqKv/85z9d1jdJKAAAgCM+uFh9bm6udu/eraioKMXFxSkyMlKrVq2y7c/Ly9O6devUoUOHS3uhYgwePFgFBQVatmyZdu/efUl9kYQCAAA44AuL1T/++ONat26d0tLS9O233+q2225TVlaWhg4dKovFopEjRyo5OVlLly7Vjh07NGzYMAUFBWnw4MEuG4cBAwbYFqXPzs5W27ZtNWDAALVo0UIffPCB0/1yTSgAAICP+v333zVo0CAdPnxY1atX1zXXXKNvvvlGsbGxkqTExERlZ2crISFBx44dU7t27bRy5UqFhIS4LIYvv/xS48aNkyQtXbpUxhgdP35c8+bN06RJk3Trrbc61S9JKAAAgCPuuNf7Rfa3aNGi8+63WCxKSkpSUlKS8zFdQGZmpsLDwyVJK1as0K233qqgoCD16tVLTzzxhNP9cjoeAAAAxYqJidHGjRt18uRJrVixwrYu6bFjxxQQEOB0v1RCAQAAHPGBSqgvGDlypO68805VqlRJsbGx6ty5s6Qzp+mbN2/udL8koQAAAChWQkKCrr76aqWnp6tbt24qV+7MifS6detq0qRJTvdLEgoAAOCAOxerL23atm2rtm3bSpIKCgq0fft2dejQQVWqVHG6T5LQMm7UkCXeDsFnvT8t1tsh+CxLOd+/E4e3mAJvR+C7TE6Ot0PwXQWhFz6mrCkopdnaZWjkyJFq3ry57rnnHhUUFNjuTR8UFKSPP/7Ydnr+YjExCQAAwBEfXKzeG/7973+rZcuWkqSPPvpIaWlp+vHHHzVy5Ejb0k3OIAkFAABwhCRUknT48GHbLUI//fRT3X777WrYsKHuuecebd++3el+SUIBAABQrIiICO3atUsFBQVasWKFbrjhBknSqVOn5Ofn53S/XBMKAADgABOTzhg+fLgGDBigqKgoWSwWdevWTZL07bffqnHjxk73SxIKAACAYiUlJalZs2ZKT0/X7bffLn9/f0mSn5+fxowZ43S/JKEAAACOsFi9zW233VakbejQoZfUJ9eEAgAA4LzWrVun3r17q379+mrQoIH69Omjr7766pL6JAkFAABwoPCaUFdvpc2CBQt0ww03KCgoSI888ogefvhhBQYGqmvXrlq4cKHT/XI6HgAAAMWaPHmyUlJS9Oijj9ra/vGPf2jatGl69tlnNXjwYKf6pRIKAADgCOuESpJ+++039e7du0h7nz59lJaW5nS/JKEAAACOkIRKkmJiYrRmzZoi7WvWrFFMTIzT/XI6HgAAAMV67LHH9Mgjj2jbtm3q0KGDLBaL1q9fr7lz5+rll192ul+SUAAAAAcs/7+5us/S5sEHH1RkZKRefPFFvffee5KkJk2aaPHixerbt6/T/ZKEAgAAwKH8/HxNnjxZ8fHxWr9+vUv75ppQAAAAR7gmVOXLl9fzzz+vgoICl/dNEgoAAIBi3XDDDfriiy9c3i+n4wEAABxwx+LypXGx+p49e2rs2LHasWOH2rRpo+DgYLv9ffr0capfklAAAAAU68EHH5QkTZs2rcg+i8Xi9Kl6klAAAABH3HENZymshFqtVrf0SxIKAABQnFKYNJYWTEwCAABAEWvXrtUVV1yhrKysIvsyMzPVtGlTffnll073TxIKAADgQOHEJFdvpcX06dN13333KTQ0tMi+sLAwjRgxQi+99JLT/ZOEAgAAoIjvv/9eN954Y7H7u3fvrs2bNzvdP9eEAgAAOFLGJyb9+eefqlChQrH7y5cvr//+979O908lFAAAAEXUrFlT27dvL3b/Dz/8oKioKKf7JwkFAABwoKxfE3rTTTdp/PjxysnJKbIvOztbEyZM0M033+x0/z6fhJ44cUIjR45UbGysAgMD1aFDB6Wmpno7LAAAgMvaU089paNHj6phw4ZKSUnRhx9+qOXLl2vq1Klq1KiRjh49qnHjxjndv89fE3rvvfdqx44dmj9/vqKjo7VgwQLdcMMN2rVrl2rWrOnt8AAAwOWqjF8TGhERoQ0bNujBBx/U2LFjZcyZ4C0Wi3r06KHXXntNERERTvfv00lodna2PvjgA3344Ye67rrrJElJSUlatmyZXn/9dU2aNMnLEQIAAFy+YmNj9emnn+rYsWP65ZdfZIxRgwYNVKVKlUvu26eT0Pz8fBUUFCggIMCuPTAwUOvXr3f4nNzcXOXm5toeO1pgFQAA4ELccQ1nabom9GxVqlTRVVdd5dI+fToJDQkJUfv27fXss8+qSZMmioiI0Lvvvqtvv/1WDRo0cPicKVOmaOLEiR6OtPS6JyzD2yH4rH9b6ng7BJ9lLD5/Obn3lNa/MB5gsotObsAZloICb4fgc3xiTMr46Xh38/m/JPPnz5cxRjVr1pS/v79eeeUVDR48WH5+fg6PHzt2rDIzM21benq6hyMGAADAhfh0JVSS6tWrp3Xr1unkyZPKyspSVFSU7rjjDsXFxTk83t/fX/7+/h6OEgAAXHaohLqVz1dCCwUHBysqKkrHjh3TZ599pr59+3o7JAAAADjJ5yuhn332mYwxatSokX755Rc98cQTatSokYYPH+7t0AAAwGWMiUnu5fOV0MzMTD300ENq3LixhgwZomuvvVYrV648771MAQAA4Nt8vhI6YMAADRgwwNthAACAsoZrQt3K5yuhAAAAOGPKlCmyWCwaOXKkrc0Yo6SkJEVHRyswMFCdO3fWzp07vRdkCZGEAgAAOGAxxi2bs1JTUzVr1iy1aNHCrj0lJUXTpk3TzJkzlZqaqsjISHXr1k0nTpy41CFwK5JQAAAAR4ybNif89ddfuvPOO/Xmm2/a3TLTGKPp06dr3Lhx6t+/v5o1a6Z58+bp1KlTWrhwoXMv5iEkoQAAAB6WlZVlt519y3FHHnroIfXq1Us33HCDXXtaWpoyMjLUvXt3W5u/v786deqkDRs2uCV2VyEJBQAAcKBwiSZXb5IUExOjsLAw2zZlypRi41i0aJG2bNni8JiMjDO3346IiLBrj4iIsO3zVT4/Ox4AAOByk56ertDQUNvj4u72mJ6ern/84x9auXKlAgICiu3PYrHYPTbGFGnzNSShAAAAjrhxiabQ0FC7JLQ4mzdv1qFDh9SmTRtbW0FBgb788kvNnDlTe/bskXSmIhoVFWU75tChQ0Wqo76G0/EAAAA+qmvXrtq+fbu2bdtm29q2bas777xT27ZtU926dRUZGalVq1bZnpOXl6d169apQ4cOXoz8wqiEAgAAOOALt+0MCQlRs2bN7NqCg4NVtWpVW/vIkSOVnJysBg0aqEGDBkpOTlZQUJAGDx7sqrDdgiQUAACgFEtMTFR2drYSEhJ07NgxtWvXTitXrlRISIi3QzsvklAAAABHfPS2nV988YXdY4vFoqSkJCUlJV165x5EEgoAAOCAL5yOv5wxMQkAAAAeRyUUAADAER89HX+5oBIKAAAAj6MSCgAAUAyu4XQfKqEAAADwOCqhAAAAjhhzZnN1n5BEJRQAAABeQCUUAADAAdYJdS+SUAAAAEdYosmtOB0PAAAAj6MSCgAA4IDFemZzdZ84g0ooAAAAPI5KKFAMS4C/t0PwWSY7x9sh+CyLsXg7BJ9l8vK8HYLPspzO93YIvqfAB8aEa0LdikooAAAAPI5KKAAAgAMs0eReVEIBAADgcVRCAQAAHOG2nW5FEgoAAOAAp+Pdi9PxAAAA8DgqoQAAAI6wRJNbUQkFAACAx1EJBQAAcIBrQt2LSigAAAA8jkooAACAIyzR5FZUQgEAAOBxVEIBAAAc4JpQ9yIJBQAAcIQlmtyK0/EAAADwOCqhAAAADnA63r2ohAIAAMDjqIQCAAA4YjVnNlf3CUlUQgEAAOAFVEIBAAAcYXa8W1EJBQAAgMdRCQUAAHDAIjfMjndtd6UaSSgAAIAj3DverTgdDwAAAI+jEgoAAOAAi9W7F5VQAAAAeByVUAAAAEdYosmtqIQCAADA40hCAQAAHLAY45btYrz++utq0aKFQkNDFRoaqvbt2+s///mPbb8xRklJSYqOjlZgYKA6d+6snTt3unoo3ILT8UBxalTzdgQ+y7L/gLdD8FmcaSue4Z7ZxbLk5Hk7BJ9jsTImklSrVi0999xzql+/viRp3rx56tu3r7Zu3aqmTZsqJSVF06ZN09y5c9WwYUNNmjRJ3bp10549exQSEuLl6M+PSigAAIAjVjdtF6F379666aab1LBhQzVs2FCTJ09WpUqV9M0338gYo+nTp2vcuHHq37+/mjVrpnnz5unUqVNauHDhJb99dyMJBQAAcMCdp+OzsrLsttzc3AvGU1BQoEWLFunkyZNq37690tLSlJGRoe7du9uO8ff3V6dOnbRhwwa3jYurkIQCAAB4WExMjMLCwmzblClTij12+/btqlSpkvz9/fXAAw9o6dKluuKKK5SRkSFJioiIsDs+IiLCts+XcU0oAACAI25coik9PV2hoaG2Zn9//2Kf0qhRI23btk3Hjx/XBx98oKFDh2rdunW2/RaL/R3pjTFF2nwRSSgAAICHFc52L4mKFSvaJia1bdtWqampevnllzV69GhJUkZGhqKiomzHHzp0qEh11BdxOh4AAMARY9yzXXJYRrm5uYqLi1NkZKRWrVpl25eXl6d169apQ4cOl/w67kYlFAAAwEc9+eST6tmzp2JiYnTixAktWrRIX3zxhVasWCGLxaKRI0cqOTlZDRo0UIMGDZScnKygoCANHjzY26FfEEkoAACAAxZzZnN1nxfjzz//1N13362DBw8qLCxMLVq00IoVK9StWzdJUmJiorKzs5WQkKBjx46pXbt2Wrlypc+vESqRhAIAAPis2bNnn3e/xWJRUlKSkpKSPBOQC5GEAgAAOOKiaziL9AlJTEwCAACAF/h0Epqfn6+nnnpKcXFxCgwMVN26dfXMM8/Iar3Ie14BAABcJIvVPRvO8OnT8VOnTtUbb7yhefPmqWnTptq0aZOGDx+usLAw/eMf//B2eAAA4HLG6Xi38ukkdOPGjerbt6969eolSapTp47effddbdq0ycuRAQAA4FL49On4a6+9VmvWrNFPP/0kSfr++++1fv163XTTTcU+Jzc3V1lZWXYbAADARTNu2iDJxyuho0ePVmZmpho3biw/Pz8VFBRo8uTJGjRoULHPmTJliiZOnOjBKAEAAHCxfLoSunjxYi1YsEALFy7Uli1bNG/ePL3wwguaN29esc8ZO3asMjMzbVt6eroHIwYAAJcLizFu2XCGT1dCn3jiCY0ZM0YDBw6UJDVv3lz79u3TlClTNHToUIfP8ff3l7+/vyfDBAAAwEXy6ST01KlTKlfOvljr5+fHEk0AAMD9mB3vVj6dhPbu3VuTJ09W7dq11bRpU23dulXTpk1TfHy8t0MDAADAJfDpJHTGjBl6+umnlZCQoEOHDik6OlojRozQ+PHjvR0aAAC43BlJrj75SiHUxqeT0JCQEE2fPl3Tp0/3digAAKCMccdEIiYm/Y9Pz44HAADA5cmnK6EAAABeY+SGiUmu7a40oxIKAAAAj6MSCgAA4AhLNLkVlVAAAAB4HJVQAAAAR6ySLG7oE5KohAIAAMALqIQCAAA4wDqh7kUSChQjs1V1b4fgs0J/P+jtEHyXhRNMxTKnvR2BzzLZ2d4OwecYa563Q2BikpvxbQkAAACPoxIKAADgCJVQt6ISCgAAAI+jEgoAAOAIlVC3ohIKAAAAj6MSCgAA4AiL1bsVlVAAAAB4HJVQAAAAB1is3r1IQgEAABxhYpJbcToeAAAAHkclFAAAwBGrkSwurlxaqYQWohIKAAAAj6MSCgAA4AjXhLoVlVAAAAB4HJVQAAAAh9xQCRWV0EJUQgEAAOBxVEIBAAAc4ZpQtyIJBQAAcMRq5PLT5yzRZMPpeAAAAHgclVAAAABHjPXM5uo+IYlKKAAAALyAJBQAAMCRwolJrt4uwpQpU3TVVVcpJCRENWrUUL9+/bRnz55zwjRKSkpSdHS0AgMD1blzZ+3cudOVI+EWJKEAAAA+at26dXrooYf0zTffaNWqVcrPz1f37t118uRJ2zEpKSmaNm2aZs6cqdTUVEVGRqpbt246ceKEFyO/MK4JBQAAcMQHZsevWLHC7vGcOXNUo0YNbd68Wdddd52MMZo+fbrGjRun/v37S5LmzZuniIgILVy4UCNGjHBZ6K5GJRQAAMDDsrKy7Lbc3NwSPS8zM1OSFB4eLklKS0tTRkaGunfvbjvG399fnTp10oYNG1wfuAuRhAIAADjixmtCY2JiFBYWZtumTJlSgnCMRo0apWuvvVbNmjWTJGVkZEiSIiIi7I6NiIiw7fNVnI4HAABwxMgNd0w685/09HSFhobamv39/S/41Icfflg//PCD1q9fX2SfxWKxfxljirT5GpJQAAAADwsNDbVLQi/k73//u5YvX64vv/xStWrVsrVHRkZKOlMRjYqKsrUfOnSoSHXU13A6HgAAwBEfWKLJGKOHH35YS5Ys0dq1axUXF2e3Py4uTpGRkVq1apWtLS8vT+vWrVOHDh1cMgzuQiUUAADARz300ENauHChPvzwQ4WEhNiu8wwLC1NgYKAsFotGjhyp5ORkNWjQQA0aNFBycrKCgoI0ePBgL0d/fiShQDH+6J7v7RB8Vuh//Lwdgu8qKPB2BL7Lwsm34pjsHG+H4HOMyfN2CJLVKsnFt9m0Xlx/r7/+uiSpc+fOdu1z5szRsGHDJEmJiYnKzs5WQkKCjh07pnbt2mnlypUKCQlxRcRuQxIKAADgo0wJTt9bLBYlJSUpKSnJ/QG5EEkoAACAI05cw1miPiGJiUkAAADwAiqhAAAAjlAJdSuSUAAAAEd84N7xlzNOxwMAAMDjqIQCAAA4YIxVxrh2iSZX91eaUQkFAACAx1EJBQAAcMQY11/DycQkGyqhAAAA8DgqoQAAAI4YN8yOpxJqQyUUAAAAHkclFAAAwBGrVbK4eDY7s+NtSEIBAAAc4XS8W3E6HgAAAB5HJRQAAMABY7XKuPh0PIvV/w+VUAAAAHgclVAAAABHuCbUraiEAgAAwOOohAIAADhiNZKFSqi7UAkFAACAx1EJBQAAcMQYSa5erJ5KaCEqoQAAAPA4KqEAAAAOGKuRcfE1oYZKqA1JKAAAgCPGKtefjmex+kKcjgcAAIDH+XwSWqdOHVksliLbQw895O3QAADAZcxYjVs2nOHzp+NTU1NVUFBge7xjxw5169ZNt99+uxejAgAAwKXw+SS0evXqdo+fe+451atXT506dfJSRAAAoEzgmlC38vkk9Gx5eXlasGCBRo0aJYvF4vCY3Nxc5ebm2h5nZmZKkrL+4oeOi2PNzvF2CD4r3+R5OwSfZcxpb4fgs4zJ93YIPquccfw3rSzL///fJW/OJs/XaZffOj5ffEcUKlVJ6LJly3T8+HENGzas2GOmTJmiiRMnFmmPvXKv+wLDZSrJ2wH4rN+9HQBwuTnl7QB815EjRxQWFubR16xYsaIiIyO1PuNTt/QfGRmpihUruqXv0sRiStGCVT169FDFihX10UcfFXvMuZXQ48ePKzY2Vvv37/f4h9jXZWVlKSYmRunp6QoNDfV2OD6FsSkeY+MY41I8xqZ4jE3xMjMzVbt2bR07dkyVK1f2+Ovn5OQoL889Z30qVqyogIAAt/RdmpSaSui+ffu0evVqLVmy5LzH+fv7y9/fv0h7WFgYv+DFCA0NZWyKwdgUj7FxjHEpHmNTPMameOXKeWchn4CAABJFN/P5JZoKzZkzRzVq1FCvXr28HQoAAAAuUalIQq1Wq+bMmaOhQ4eqfPlSU7wFAABAMUpFErp69Wrt379f8fHxF/1cf39/TZgwweEp+rKOsSkeY1M8xsYxxqV4jE3xGJviMTaXv1I1MQkAAACXh1JRCQUAAMDlhSQUAAAAHkcSCgAAAI8jCQUAAIDHlYkkNDc3V61atZLFYtG2bdvs9u3fv1+9e/dWcHCwqlWrpkceecRtd0jwJX369FHt2rUVEBCgqKgo3X333frjjz/sjklNTVXXrl1VuXJlValSRd27dy8yfpejkoyNJM2dO1ctWrRQQECAIiMj9fDDD3shWs8p6bhIZ26zV6tWLVksFh0/ftyzgXrBhcbm+++/16BBgxQTE6PAwEA1adJEL7/8shcj9pySfG7K4vfw3r17dc899yguLk6BgYGqV6+eJkyYUOR9l8Xv4ZKOjVT2vocvN2UiCU1MTFR0dHSR9oKCAvXq1UsnT57U+vXrtWjRIn3wwQd67LHHvBClZ3Xp0kXvvfee9uzZow8++EC//vqrbrvtNtv+EydOqEePHqpdu7a+/fZbrV+/XqGhoerRo4dOnz7txcjd70JjI0nTpk3TuHHjNGbMGO3cuVNr1qxRjx49vBSxZ5RkXArdc889atGihYcj9J4Ljc3mzZtVvXp1LViwQDt37tS4ceM0duxYzZw504tRe8aFxqasfg//+OOPslqt+uc//6mdO3fqpZde0htvvKEnn3zSdkxZ/R4uydhIZfN7+LJjLnOffvqpady4sdm5c6eRZLZu3Wq3r1y5cubAgQO2tnfffdf4+/ubzMxML0TrPR9++KGxWCwmLy/PGGNMamqqkWT2799vO+aHH34wkswvv/zirTC94tyxOXr0qAkMDDSrV6/2cmTede64FHrttddMp06dzJo1a4wkc+zYMe8E6EXFjc3ZEhISTJcuXTwYlW84d2z4Hv6flJQUExcXZ3vM9/D/nDs2fA9fHi7rSuiff/6p++67T/Pnz1dQUFCR/Rs3blSzZs3sqqQ9evRQbm6uNm/e7MlQvero0aP617/+pQ4dOqhChQqSpEaNGqlatWqaPXu28vLylJ2drdmzZ6tp06aKjY31csSe42hsVq1aJavVqgMHDqhJkyaqVauWBgwYoPT0dC9H6zmOxkWSdu3apWeeeUbvvPOO1+737G3Fjc25MjMzFR4e7sHIvM/R2PA9/D/nfib4Hv6fc8eG7+HLw2X7V8IYo2HDhumBBx5Q27ZtHR6TkZGhiIgIu7YqVaqoYsWKysjI8ESYXjV69GgFBweratWq2r9/vz788EPbvpCQEH3xxRdasGCBAgMDValSJX322Wf69NNPy8StU883Nr/99pusVquSk5M1ffp0/fvf/9bRo0fVrVu3y/46tvONS25urgYNGqTnn39etWvX9mKU3nG+sTnXxo0b9d5772nEiBEejNB7zjc2Zf17uNCvv/6qGTNm6IEHHrC1lfXv4UKOxqYsfw9fTkpdEpqUlCSLxXLebdOmTZoxY4aysrI0duzY8/ZnsViKtBljHLb7upKOTaEnnnhCW7du1cqVK+Xn56chQ4bI/P8NtLKzsxUfH6+OHTvqm2++0ddff62mTZvqpptuUnZ2trfeotNcOTZWq1WnT5/WK6+8oh49euiaa67Ru+++q59//lmff/65t96iU1w5LmPHjlWTJk101113eevtuJQrx+ZsO3fuVN++fTV+/Hh169bNk2/JZVw9NmX5e1iS/vjjD9144426/fbbde+999ray/r3sFT82FxO38NlWam7befhw4d1+PDh8x5Tp04dDRw4UB999JHdl1hBQYH8/Px05513at68eRo/frw+/PBDff/997Zjjh07pvDwcK1du1ZdunRx2/twh5KOTUBAQJH233//XTExMdqwYYPat2+v2bNn68knn9TBgwdtp1Xz8vJUpUoVzZ49WwMHDnTLe3AXV47NnDlzFB8fr/T0dNWqVct2XEREhCZNmqT77rvP5fG7iyvHpVWrVtq+fbvtd84YI6vVKj8/P40bN04TJ050y3twF1eOTaFdu3apS5cuuvfeezV58mSXx+wprhybsv49/Mcff6hLly5q166d5s6da3cZS1n/Hj7f2FxO38NlWamr51erVk3VqlW74HGvvPKKJk2aZHv8xx9/qEePHlq8eLHatWsnSWrfvr0mT56sgwcPKioqSpK0cuVK+fv7q02bNu55A25U0rFxpPDfIrm5uZKkU6dOqVy5cnZJfOFjq9V66cF6mCvHpmPHjpKkPXv22L78jh49qsOHD5e667RcOS4ffPCBXXUmNTVV8fHx+uqrr1SvXr1LD9bDXDk20pkK6PXXX6+hQ4eW6gRUcu3YlOXv4QMHDqhLly5q06aN5syZU+Q66rL8PXyhsbmcvofLNM/PhfKOtLS0IrPj8/PzTbNmzUzXrl3Nli1bzOrVq02tWrXMww8/7L1APeDbb781M2bMMFu3bjV79+41a9euNddee62pV6+eycnJMcYYs3v3buPv728efPBBs2vXLrNjxw5z1113mbCwMPPHH394+R24T0nGxhhj+vbta5o2bWq+/vprs337dnPzzTebK6644ryzoUuzko7L2T7//PMyMTu+JGOzY8cOU716dXPnnXeagwcP2rZDhw55OXr3KsnYlNXv4QMHDpj69eub66+/3vz+++92n4tCZfV7uCRjY0zZ+x6+HJXpJNQYY/bt22d69eplAgMDTXh4uHn44YeL/aN6ufjhhx9Mly5dTHh4uPH39zd16tQxDzzwgPn999/tjlu5cqXp2LGjCQsLM1WqVDHXX3+92bhxo5ei9oySjk1mZqaJj483lStXNuHh4eaWW26xW0blclPScTlbWUlCSzI2EyZMMJKKbLGxsd4L3ANK+rkpi9/Dc+bMcfiZOLc2VBa/h0s6NmXte/hyVOquCQUAAEDpV+pmxwMAAKD0IwkFAACAx5GEAgAAwONIQgEAAOBxJKEAAADwOJJQAAAAeBxJKAAAADyOJBQAAAAeRxIKAAAAjyMJBeA2GRkZ+sc//qH69esrICBAERERuvbaa/XGG2/o1KlT3g4PAOBF5b0dAIDL02+//aaOHTuqcuXKSk5OVvPmzZWfn6+ffvpJb7/9tqKjo9WnTx9vhwkA8BIqoQDcIiEhQeXLl9emTZs0YMAANWnSRM2bN9ett96qTz75RL1795YkTZs2Tc2bN1dwcLBiYmKUkJCgv/76y9bP3LlzVblyZX388cdq1KiRgoKCdNttt+nkyZOaN2+e6tSpoypVqujvf/+7CgoKbM+rU6eOJk2apCFDhqhSpUqKjY3Vhx9+qP/+97/q27evKlWqpObNm2vTpk225xw5ckSDBg1SrVq1FBQUpObNm+vdd9/13KABQBlCEgrA5Y4cOaKVK1fqoYceUnBwsMNjLBaLJKlcuXJ65ZVXtGPHDs2bN09r165VYmKi3bGnTp3SK6+8okWLFmnFihX64osv1L9/f3366af69NNPNX/+fM2aNUv//ve/7Z730ksvqWPHjtq6dat69eqlu+++W0OGDNFdd92lLVu2qH79+hoyZIiMMZKknJwctWnTRh9//LF27Nih+++/X3fffbe+/fZbN4wSAJRxBgBc7JtvvjGSzJIlS+zaq1ataoKDg01wcLBJTEx0+Nz33nvPVK1a1fZ4zpw5RpL55ZdfbG0jRowwQUFB5sSJE7a2Hj16mBEjRtgex8bGmrvuusv2+ODBg0aSefrpp21tGzduNJLMwYMHi30vN910k3nsscdK8K4BABeDa0IBuE1htbPQd999J6vVqjvvvFO5ubmSpM8//1zJycnatWuXsrKylJ+fr5ycHJ08edJWRQ0KClK9evVs/URERKhOnTqqVKmSXduhQ4fsXq9FixZ2+yWpefPmRdoOHTqkyMhIFRQU6LnnntPixYt14MAB5ebmKjc3t9hqLgDAeZyOB+By9evXl8Vi0Y8//mjXXrduXdWvX1+BgYGSpH379ummm25Ss2bN9MEHH2jz5s169dVXJUmnT5+2Pa9ChQp2/VgsFodtVqvVru3sYwoTYkdthc978cUX9dJLLykxMVFr167Vtm3b1KNHD+Xl5V38IAAAzoskFIDLVa1aVd26ddPMmTN18uTJYo/btGmT8vPz9eKLL+qaa65Rw4YN9ccff3gwUntfffWV+vbtq7vuukstW7ZU3bp19fPPP3stHgC4nJGEAnCL1157Tfn5+Wrbtq0WL16s3bt3a8+ePVqwYIF+/PFH+fn5qV69esrPz9eMGTP022+/af78+XrjjTe8FnP9+vW1atUqbdiwQbt379aIESOUkZHhtXgA4HJGEgrALerVq6etW7fqhhtu0NixY9WyZUu1bdtWM2bM0OOPP65nn31WrVq10rRp0zR16lQ1a9ZM//rXvzRlyhSvxfz000/ryiuvVI8ePdS5c2dFRkaqX79+XosHAC5nFmP+f20SAAAAwEOohAIAAMDjSEIBAADgcSShAAAA8DiSUAAAAHgcSSgAAAA8jiQUAAAAHkcSCgAAAI8jCQUAAIDHkYQCAADA40hCAQAA4HEkoQAAAPC4/wOY7yhmulJDOAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO 3D plot?\n",
    "Gamma, Sigma = np.meshgrid(log2_gammas, log2_sigma)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cv_errors, interpolation='nearest', cmap='viridis', \n",
    "           extent=[log2_gammas.min(), log2_gammas.max(), log2_sigma.min(), log2_sigma.max()],\n",
    "           aspect='auto', origin='lower', vmin=cv_errors.min(), vmax=100)\n",
    "plt.colorbar(label='Cross-validation Error')\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "plt.xlabel('Gamma')\n",
    "plt.ylabel('Sigma')\n",
    "plt.title('Cross-validation Error as function of Gamma and Sigma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 c) Calculate the MSE on the training and test sets for the best γ and σ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score 8.184009734874135\n",
      "Test set score 11.874773255198415\n"
     ]
    }
   ],
   "source": [
    "# Retrain the model with best sigma and best gamma on the entire training set\n",
    "# X, Y\n",
    "# Train the model on the training data\n",
    "K_train = compute_kernel_matrix(main_train_X, main_train_X, chosen_sigma) \n",
    "alpha = KernelRidgeRegressionCustom(K_train, main_train_Y, chosen_sigma, chosen_gamma)\n",
    "\n",
    "# Evaluate on the training set\n",
    "y_pred = predict_KernelRidgeRegressionCustom(main_train_X, main_train_X, chosen_sigma, alpha)\n",
    "trainscore = compute_mse(main_train_Y, y_pred)\n",
    "print('Training set score',trainscore)\n",
    "\n",
    "# Evaluate on the test set # testX, testY\n",
    "y_pred = predict_KernelRidgeRegressionCustom(testX, main_train_X, chosen_sigma, alpha)\n",
    "testscore = compute_mse(testY, y_pred)\n",
    "print('Test set score',testscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0/20: \n",
      "-Computing kernel matrices ...\n",
      "--Iterating over gammas and sigmas\n",
      "--- Best gamma and sigma for the run : 4.656612873077393e-10 512 12.400608556782602\n",
      "---- Training set score 6.7943199415164255\n",
      "---- Test set score 12.677722999212326\n",
      "Run 1/20: \n",
      "-Computing kernel matrices ...\n",
      "--Iterating over gammas and sigmas\n",
      "--- Best gamma and sigma for the run : 3.725290298461914e-09 512 12.699266265465678\n",
      "---- Training set score 8.0937370239452\n",
      "---- Test set score 11.968982977804892\n",
      "Run 2/20: \n",
      "-Computing kernel matrices ...\n",
      "--Iterating over gammas and sigmas\n",
      "--- Best gamma and sigma for the run : 1.862645149230957e-09 512 10.87325709915405\n",
      "---- Training set score 6.656517531554831\n",
      "---- Test set score 13.581734690165302\n",
      "Run 3/20: \n",
      "-Computing kernel matrices ...\n",
      "--Iterating over gammas and sigmas\n",
      "--- Best gamma and sigma for the run : 9.094947017729282e-13 1024 12.109042540634416\n",
      "---- Training set score 5.313556783388848\n",
      "---- Test set score 12.417498409989895\n",
      "Run 4/20: \n",
      "-Computing kernel matrices ...\n",
      "--Iterating over gammas and sigmas\n",
      "--- Best gamma and sigma for the run : 3.725290298461914e-09 512 14.852262379080369\n",
      "---- Training set score 8.239258775078417\n",
      "---- Test set score 11.770225604594465\n",
      "Run 5/20: \n",
      "-Computing kernel matrices ...\n",
      "--Iterating over gammas and sigmas\n",
      "--- Best gamma and sigma for the run : 9.094947017729282e-13 4096 12.245870588820821\n",
      "---- Training set score 8.7362133157108\n",
      "---- Test set score 13.477950903146036\n",
      "Run 6/20: \n",
      "-Computing kernel matrices ...\n",
      "--Iterating over gammas and sigmas\n",
      "--- Best gamma and sigma for the run : 1.4901161193847656e-08 256 10.662971925063687\n",
      "---- Training set score 6.342838930691697\n",
      "---- Test set score 14.513898023564769\n",
      "Run 7/20: \n",
      "-Computing kernel matrices ...\n",
      "--Iterating over gammas and sigmas\n",
      "--- Best gamma and sigma for the run : 4.656612873077393e-10 512 13.493949607406297\n",
      "---- Training set score 7.337251212277216\n",
      "---- Test set score 11.564839085962353\n",
      "Run 8/20: \n",
      "-Computing kernel matrices ...\n",
      "--Iterating over gammas and sigmas\n",
      "--- Best gamma and sigma for the run : 2.3283064365386963e-10 256 10.967792471683966\n",
      "---- Training set score 3.3749872815655957\n",
      "---- Test set score 20.65660219104254\n",
      "Run 9/20: \n",
      "-Computing kernel matrices ...\n",
      "--Iterating over gammas and sigmas\n",
      "--- Best gamma and sigma for the run : 2.9103830456733704e-11 2048 12.84792578763342\n",
      "---- Training set score 9.510328905998657\n",
      "---- Test set score 11.865634192196426\n",
      "Run 10/20: \n",
      "-Computing kernel matrices ...\n",
      "--Iterating over gammas and sigmas\n",
      "--- Best gamma and sigma for the run : 2.3283064365386963e-10 512 11.582686261357344\n",
      "---- Training set score 6.415849661080564\n",
      "---- Test set score 12.487906103788207\n",
      "Run 11/20: \n",
      "-Computing kernel matrices ...\n",
      "--Iterating over gammas and sigmas\n",
      "--- Best gamma and sigma for the run : 9.094947017729282e-13 4096 10.152964507605883\n",
      "---- Training set score 7.045680097419941\n",
      "---- Test set score 17.944596402640226\n",
      "Run 12/20: \n",
      "-Computing kernel matrices ...\n",
      "--Iterating over gammas and sigmas\n",
      "--- Best gamma and sigma for the run : 3.725290298461914e-09 512 14.350680680911363\n",
      "---- Training set score 9.455382420430237\n",
      "---- Test set score 8.559843155864666\n",
      "Run 13/20: \n",
      "-Computing kernel matrices ...\n",
      "--Iterating over gammas and sigmas\n",
      "--- Best gamma and sigma for the run : 9.094947017729282e-13 2048 11.822198423144187\n",
      "---- Training set score 7.354441283082864\n",
      "---- Test set score 13.52890556555701\n",
      "Run 14/20: \n",
      "-Computing kernel matrices ...\n",
      "--Iterating over gammas and sigmas\n",
      "--- Best gamma and sigma for the run : 1.862645149230957e-09 512 11.758459043740078\n",
      "---- Training set score 7.906330253440132\n",
      "---- Test set score 10.148445948729279\n",
      "Run 15/20: \n",
      "-Computing kernel matrices ...\n",
      "--Iterating over gammas and sigmas\n",
      "--- Best gamma and sigma for the run : 3.725290298461914e-09 512 14.06482959372743\n",
      "---- Training set score 9.12352253885478\n",
      "---- Test set score 8.337444933502967\n",
      "Run 16/20: \n",
      "-Computing kernel matrices ...\n",
      "--Iterating over gammas and sigmas\n",
      "--- Best gamma and sigma for the run : 1.8189894035458565e-12 4096 16.017439989680746\n",
      "---- Training set score 9.638620620828181\n",
      "---- Test set score 11.722313111104103\n",
      "Run 17/20: \n",
      "-Computing kernel matrices ...\n",
      "--Iterating over gammas and sigmas\n",
      "--- Best gamma and sigma for the run : 4.656612873077393e-10 1024 14.735187809941433\n",
      "---- Training set score 10.251446883409448\n",
      "---- Test set score 9.953614615516614\n",
      "Run 18/20: \n",
      "-Computing kernel matrices ...\n",
      "--Iterating over gammas and sigmas\n",
      "--- Best gamma and sigma for the run : 3.725290298461914e-09 512 13.026433360196757\n",
      "---- Training set score 8.138641255490194\n",
      "---- Test set score 12.645341512313582\n",
      "Run 19/20: \n",
      "-Computing kernel matrices ...\n",
      "--Iterating over gammas and sigmas\n",
      "--- Best gamma and sigma for the run : 3.637978807091713e-12 4096 14.077055477986471\n",
      "---- Training set score 10.884338966035683\n",
      "---- Test set score 9.359959543424003\n"
     ]
    }
   ],
   "source": [
    "# Repetition of 4 a,c,d already done above \n",
    "# Below code is Repetition of 5 a and c 20 times\n",
    "\n",
    "gamma_values = np.array([2**-i for i in range(40, 25, -1)])\n",
    "sigma_values = np.array([2**i for i in range(7, 14)])\n",
    "best_20_gammas = []\n",
    "best_20_sigmas = []\n",
    "train_errors_chosenparams = []\n",
    "test_errors_chosenparams = []\n",
    "\n",
    "for repeat in range(20): #TODO:20\n",
    "    print(f'Run {repeat}/20: ')\n",
    "    # Split data into train and test\n",
    "    big_train_X, big_train_Y, testX, testY = get_train_test(df)\n",
    "\n",
    "    # Create folds for cross validation\n",
    "    n_folds = 5\n",
    "    fold_size = int(len(big_train_X)/n_folds)\n",
    "    fold_indices = [] \n",
    "    indices = np.arange(len(big_train_X))\n",
    "    for i in range(n_folds):\n",
    "        start_idx, end_idx = i * fold_size,  (i + 1) * fold_size\n",
    "        validation_indices = indices[start_idx: end_idx]\n",
    "        train_indices = np.concatenate([indices[:start_idx], indices[end_idx:]])\n",
    "        fold_indices.append((train_indices, validation_indices))\n",
    "\n",
    "    # dictionary to store kernel matrices for each training fold for each sigma - 5 folds * 7 sigmas\n",
    "    print('-Computing kernel matrices ...')\n",
    "    kernel_matrices = {}\n",
    "    for fold_num, (train_indices, validation_indices) in enumerate(fold_indices):\n",
    "        x_train_fold = big_train_X[train_indices]\n",
    "        # Precompute and store kernel matrix for each sigma for this fold\n",
    "        for sigma in sigma_values:\n",
    "            fold_sigma_key = (fold_num, sigma)\n",
    "            kernel_matrices[fold_sigma_key] = compute_kernel_matrix(x_train_fold,x_train_fold, sigma) \n",
    "\n",
    "    chosen_gamma, chosen_sigma = None, None\n",
    "    best_score = float('inf')\n",
    "\n",
    "    cv_errors = np.zeros((len(gamma_values), len(sigma_values)))\n",
    "\n",
    "    # 525 total - 5 folds * 7 sigma * 15 gamma\n",
    "    print('--Iterating over gammas and sigmas')\n",
    "    for i, gamma in enumerate(gamma_values):\n",
    "        for j, sigma in enumerate(sigma_values):\n",
    "            fold_scores = []\n",
    "\n",
    "            for fold_num, (train_indices, validation_indices) in enumerate(fold_indices):\n",
    "                x_train, y_train = big_train_X[train_indices], big_train_Y[train_indices]\n",
    "                x_validation, y_validation = big_train_X[validation_indices], big_train_Y[validation_indices]\n",
    "                # Retrieve the precomputed kernel matrix for this fold and sigma\n",
    "                K_train = kernel_matrices[(fold_num, sigma)]\n",
    "                \n",
    "                # Train the model on the training data\n",
    "                alpha = KernelRidgeRegressionCustom(K_train, y_train, sigma, gamma)\n",
    "                \n",
    "                # Make predictions on the validation data\n",
    "                y_pred = predict_KernelRidgeRegressionCustom(x_validation, x_train, sigma, alpha)\n",
    "                score = compute_mse(y_validation, y_pred)\n",
    "                fold_scores.append(score)\n",
    "                # print(y_validation, y_pred)\n",
    "\n",
    "            average_score = np.mean(fold_scores)\n",
    "            # print(f'gamma: {gamma}, sigma: {sigma}, scores: {average_score}')\n",
    "            cv_errors[i, j] = average_score\n",
    "\n",
    "            if average_score < best_score:\n",
    "                best_score = average_score\n",
    "                chosen_gamma, chosen_sigma = gamma, sigma\n",
    "    print('--- Best gamma and sigma for the run :',chosen_gamma,chosen_sigma,best_score)\n",
    "    best_20_gammas.append(chosen_gamma)\n",
    "    best_20_sigmas.append(chosen_sigma)\n",
    "\n",
    "    # Retrain the model with best sigma and best gamma on the entire training set big_train_X, big_train_Y\n",
    "    # Train the model on the training data\n",
    "    K_train = compute_kernel_matrix(big_train_X,big_train_X, chosen_sigma) \n",
    "    alpha = KernelRidgeRegressionCustom(K_train, big_train_Y, chosen_sigma, chosen_gamma)\n",
    "    # Evaluate on the training set\n",
    "    y_pred = predict_KernelRidgeRegressionCustom(big_train_X, big_train_X, chosen_sigma, alpha)\n",
    "    trainscore = compute_mse(big_train_Y, y_pred)\n",
    "    print('---- Training set score',trainscore)\n",
    "    train_errors_chosenparams.append(trainscore)\n",
    "    # Evaluate on the test set\n",
    "    # testX, testY\n",
    "    y_pred = predict_KernelRidgeRegressionCustom(testX, big_train_X, chosen_sigma, alpha)\n",
    "    testscore = compute_mse(testY, y_pred)\n",
    "    print('---- Test set score',testscore)\n",
    "    test_errors_chosenparams.append(testscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of train data:  mean= 7.830663184089985,  sd=1.741928913924968\n",
      "MSE of test data:  mean= 12.459172998505984,  sd=2.836043166725966\n"
     ]
    }
   ],
   "source": [
    "# KRR MSE Train and Test - mean and sd\n",
    "print(f'MSE of train data:  mean= {np.mean(train_errors_chosenparams)},  sd={np.std(train_errors_chosenparams)}')\n",
    "print(f'MSE of test data:  mean= {np.mean(test_errors_chosenparams)},  sd={np.std(test_errors_chosenparams)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-31. -28. -29. -40. -28. -40. -26. -31. -32. -35. -32. -40. -28. -40.\n",
      " -29. -28. -39. -31. -28. -38.]\n",
      "[ 9.  9.  9. 10.  9. 12.  8.  9.  8. 11.  9. 12.  9. 11.  9.  9. 12. 10.\n",
      "  9. 12.]\n"
     ]
    }
   ],
   "source": [
    "print(np.log2(best_20_gammas))\n",
    "print(np.log2(best_20_sigmas))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "supervisedl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
